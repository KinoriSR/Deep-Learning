{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.datasets import ImageFolder\n",
    "# from torchvision.transforms import ToTensor\n",
    "import torchvision.transforms as transforms\n",
    "from torch.nn import Module\n",
    "from torch.nn import Linear, CosineSimilarity, BatchNorm1d, BatchNorm2d, Sequential, Dropout, ReLU, LeakyReLU, Conv2d, MaxPool2d, AvgPool2d, AdaptiveAvgPool2d, CrossEntropyLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda = True\n",
    "batchsize = 256 if cuda else 64\n",
    "num_workers = 4 if cuda else 0\n",
    "# from Recitation 5\n",
    "# not doing horizontal flip as features can be side dependent, not doing RandomGrayScale to start\n",
    "transformations = transforms.Compose([\n",
    "#     transforms.Resize((68,68)),\n",
    "#     transforms.RandomCrop((64,64)), # this should help with translation variants\n",
    "#     transforms.ColorJitter(brightness=0.09, contrast=0.09, saturation=0.09),  \n",
    "#     transforms.RandomRotation(degrees=5), #20\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "train_dataset = ImageFolder(root='classification_data/train_data/', transform=transformations)#ToTensor())\n",
    "droplast=False\n",
    "if len(train_dataset)%batchsize==1:\n",
    "    print(\"Dropping last because last batch of size:\", len(train_dataset)%batchsize)\n",
    "    droplast=True\n",
    "train_loader_args = dict(batch_size=batchsize, shuffle=True, num_workers=num_workers, pin_memory=True, drop_last=droplast) if cuda else dict(shuffle=True, batch_size=batchsize, drop_last=droplast)\n",
    "train_loader = DataLoader(train_dataset, **train_loader_args)\n",
    "\n",
    "val_dataset = ImageFolder(root='classification_data/val_data/', transform=transforms.ToTensor())\n",
    "val_loader_args = dict(batch_size=batchsize, shuffle=False, num_workers=num_workers, pin_memory=True) if cuda else dict(shuffle=False, batch_size=batchsize)\n",
    "val_loader = DataLoader(val_dataset, **val_loader_args)\n",
    "\n",
    "class VerificationMyDataset(Dataset):\n",
    "    def __init__(self, pair_path, test=False):\n",
    "        pairs = open(pair_path, \"r\").readlines(-1)\n",
    "        self.test = test\n",
    "        self.x=[]\n",
    "        if not self.test:\n",
    "            self.y=[]\n",
    "        self.to_tensor = transforms.ToTensor()\n",
    "        self.length=len(pairs)\n",
    "        for i in range(self.length):\n",
    "            line = pairs[i].strip().split(' ')\n",
    "            image1 = self.to_tensor(Image.open(line[0]))\n",
    "            image2 = self.to_tensor(Image.open(line[1]))\n",
    "            self.x.append((image1,image2))\n",
    "            if not self.test:\n",
    "                self.y.append(int(line[2]))\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    # keep this simple/quick because run many times\n",
    "    def __getitem__(self, index):\n",
    "        if not self.test:\n",
    "            return self.x[index], self.y[index]\n",
    "        else:\n",
    "            return self.x[index]\n",
    "pairs_path = \"verification_pairs_val.txt\"\n",
    "\n",
    "ver_val_dataset = VerificationMyDataset(pairs_path)\n",
    "ver_val_loader_args = dict(batch_size=batchsize, shuffle=False, num_workers=num_workers, pin_memory=True) if cuda else dict(shuffle=False, batch_size=batchsize)\n",
    "ver_val_loader = DataLoader(ver_val_dataset, **ver_val_loader_args)\n",
    "\n",
    "# test_data = ImageFolder(root='classification_data/test_data/', transform=ToTensor())\n",
    "# test_loader = DataLoader(test_data, batch_size=batchsize, shuffle=True, num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 64, 64])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[1000][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(108520, 341782, 0)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train_dataset[0][0].shape\n",
    "pairs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## On Deck:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model_24_ \n",
    "class Model(Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # HEAD\n",
    "        head = [\n",
    "            Conv2d(3, 64, kernel_size=3, stride=1, bias=False), # 64x64\n",
    "            MaxPool2d(kernel_size=3, stride=2, padding=1) # 32x32\n",
    "        ]\n",
    "        self.head = Sequential(*head)\n",
    "        \n",
    "        # Embedder\n",
    "        self.block1 = BasicBlock(64,64)\n",
    "        self.block2 = BasicBlock(64,64)\n",
    "        \n",
    "        self.block3 = BasicBlock(64,128, stride=2) # 16x16\n",
    "        self.skipdown1 = SkipDownSample(64,128, stride=2)             # MAYBE SHIFT THIS DOWN 1 SET OF BLOCKS\n",
    "        self.block4 = BasicBlock(128,128)\n",
    "        \n",
    "        self.block5 = BasicBlock(128,256) # , stride=2) # 16x16\n",
    "        self.skipdown2 = SkipDownSample(128,256) # , stride=2)\n",
    "        self.block6 = BasicBlock(256,256)\n",
    "        \n",
    "        self.block7 = BasicBlock(256,512, stride=2) # 8x8\n",
    "        self.skipdown3 = SkipDownSample(256,512, stride=2)            # MAYBE SHIFT THIS DOWN 1 SET OF BLOCKS\n",
    "        self.block8 = BasicBlock(512,512)\n",
    "        \n",
    "        # I added\n",
    "        self.block9 = BasicBlock(512, 1024) # 8x8\n",
    "        self.skipdown4 = SkipDownSample(512,1024)\n",
    "        \n",
    "        self.block10 = BasicBlock(1024, 2048) # 8x8\n",
    "        self.skipdown5 = SkipDownSample(1024,2048)\n",
    "\n",
    "        # EMBEDDING\n",
    "        self.avgpool = AvgPool2d(kernel_size=8, stride=1) # kernel size 4 -> 8\n",
    "        \n",
    "        # CLASSIFICATION\n",
    "        self.classification = Linear(2048, 4000)\n",
    "        #             Linear(1024, 4000)\n",
    "\n",
    "        \n",
    "#         ]\n",
    "#         self.classification = Sequential(*classification)\n",
    "    \n",
    "    \n",
    "    def forward(self, x):\n",
    "        # HEAD\n",
    "        # 64x64\n",
    "        out = self.head(x)\n",
    "        # 32x32\n",
    "        # EMBEDDING\n",
    "        out = self.block1(out) + out\n",
    "        out = self.block2(out) + out\n",
    "        out = self.block3(out) + self.skipdown1(out) # 16x16\n",
    "        out = self.block4(out) + out\n",
    "        out = self.block5(out) + self.skipdown2(out)# 8x8\n",
    "        out = self.block6(out) + out\n",
    "        out = self.block7(out) + self.skipdown3(out)# 4x4\n",
    "        out = self.block8(out) + out\n",
    "        out = self.block9(out) + self.skipdown4(out)# 2x2\n",
    "        out = self.block10(out) + self.skipdown5(out)\n",
    "        \n",
    "        out = self.avgpool(out)\n",
    "        embedding = torch.flatten(out,1)\n",
    "        \n",
    "        classification = self.classification(embedding)\n",
    "                \n",
    "        return embedding, classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model_18_\n",
    "class BasicBlock(Module): \n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, dropout=False, drop_rate=0.3): # groups=1, dilation=1, norm_layer=None\n",
    "        super().__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        layers = [\n",
    "            Conv2d(in_channels, out_channels, kernel_size=kernel_size, padding=kernel_size-2, stride=stride, bias=False),  # , padding=kernel_size-1\n",
    "            BatchNorm2d(out_channels),\n",
    "            ReLU(),\n",
    "            Conv2d(out_channels, out_channels, kernel_size=kernel_size, padding=kernel_size-2, stride=1, bias=False),  # , padding=kernel_size-1\n",
    "            BatchNorm2d(out_channels),\n",
    "            ReLU()\n",
    "        ]\n",
    "        \n",
    "        if dropout:\n",
    "            layers.append(Dropout(drop_rate))\n",
    "            \n",
    "        self.block = Sequential(*layers)\n",
    "\n",
    "    def forward(self,x):\n",
    "        out = self.block(x)\n",
    "         \n",
    "        return out\n",
    "\n",
    "class SkipDownSample(Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1): # groups=1, dilation=1, norm_layer=None\n",
    "        super().__init__()\n",
    "    \n",
    "#     layers = [\n",
    "#         Conv2d(in_channels, out_channels, kernel_size=1, stride=1, bias=False)\n",
    "#     ]\n",
    "    \n",
    "        self.skip = Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False)\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.skip(x)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 64, 32, 32])\n",
      "torch.Size([32, 64, 32, 32])\n",
      "torch.Size([32, 64, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "block=BasicBlock(64,64,stride=2)\n",
    "data=torch.rand((32,64,64,64))\n",
    "print(block.forward(data).shape)\n",
    "\n",
    "skip = SkipDownSample(64,64,stride=2)\n",
    "data=torch.rand((32,64,64,64))\n",
    "print(skip.forward(data).shape)\n",
    "\n",
    "maxpool=MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "data=torch.rand((32,64,64,64))\n",
    "print(maxpool.forward(data).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model(\n",
      "  (head): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
      "    (1): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (block1): BasicBlock(\n",
      "    (block): Sequential(\n",
      "      (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU()\n",
      "      (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (5): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (block2): BasicBlock(\n",
      "    (block): Sequential(\n",
      "      (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU()\n",
      "      (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (5): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (block3): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (block): Sequential(\n",
      "        (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU()\n",
      "        (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (5): ReLU()\n",
      "      )\n",
      "    )\n",
      "    (1): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (skipdown1): SkipDownSample(\n",
      "    (skip): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "  )\n",
      "  (block4): BasicBlock(\n",
      "    (block): Sequential(\n",
      "      (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU()\n",
      "      (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (5): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (block5): BasicBlock(\n",
      "    (block): Sequential(\n",
      "      (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU()\n",
      "      (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (5): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (skipdown2): SkipDownSample(\n",
      "    (skip): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "  )\n",
      "  (block6): BasicBlock(\n",
      "    (block): Sequential(\n",
      "      (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU()\n",
      "      (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (5): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (block7): BasicBlock(\n",
      "    (block): Sequential(\n",
      "      (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU()\n",
      "      (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (5): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (skipdown3): SkipDownSample(\n",
      "    (skip): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "  )\n",
      "  (block8): BasicBlock(\n",
      "    (block): Sequential(\n",
      "      (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU()\n",
      "      (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (5): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (block9): BasicBlock(\n",
      "    (block): Sequential(\n",
      "      (0): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU()\n",
      "      (3): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (4): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (5): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (skipdown4): SkipDownSample(\n",
      "    (skip): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "  )\n",
      "  (block10): BasicBlock(\n",
      "    (block): Sequential(\n",
      "      (0): Conv2d(1024, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU()\n",
      "      (3): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (4): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (5): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (skipdown5): SkipDownSample(\n",
      "    (skip): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "  )\n",
      "  (avgpool): AvgPool2d(kernel_size=8, stride=1, padding=0)\n",
      "  (classification): Linear(in_features=2048, out_features=4000, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Model_26_ \n",
    "class Model(Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # HEAD\n",
    "        head = [\n",
    "            Conv2d(3, 64, kernel_size=3, stride=1, bias=False), # 64x64\n",
    "            MaxPool2d(kernel_size=3, stride=2, padding=1) # 32x32\n",
    "        ]\n",
    "        self.head = Sequential(*head)\n",
    "        \n",
    "        # Embedder\n",
    "        self.block1 = BasicBlock(64,64)\n",
    "        self.block2 = BasicBlock(64,64)\n",
    "        \n",
    "        self.block3 = Sequential(BasicBlock(64,128), MaxPool2d(kernel_size=3, stride=2, padding=1))#, stride=2) # 16x16\n",
    "        self.skipdown1 = SkipDownSample(64,128, stride=2)             # MAYBE SHIFT THIS DOWN 1 SET OF BLOCKS\n",
    "        self.block4 = BasicBlock(128,128)\n",
    "        \n",
    "        self.block5 = BasicBlock(128,256) # , stride=2) # 16x16\n",
    "        self.skipdown2 = SkipDownSample(128,256) # , stride=2)\n",
    "        self.block6 = BasicBlock(256,256)\n",
    "        \n",
    "        self.block7 = BasicBlock(256,512, stride=2) # 8x8\n",
    "        self.skipdown3 = SkipDownSample(256,512, stride=2)            # MAYBE SHIFT THIS DOWN 1 SET OF BLOCKS\n",
    "        self.block8 = BasicBlock(512,512)\n",
    "        \n",
    "        # I added\n",
    "        self.block9 = BasicBlock(512, 1024) # 8x8\n",
    "        self.skipdown4 = SkipDownSample(512,1024)\n",
    "        \n",
    "        self.block10 = BasicBlock(1024,2048) # 8x8\n",
    "        self.skipdown5 = SkipDownSample(1024,2048)\n",
    "\n",
    "        # EMBEDDING\n",
    "        self.avgpool = AvgPool2d(kernel_size=8, stride=1) # kernel size 4 -> 8\n",
    "        \n",
    "        # CLASSIFICATION\n",
    "        self.classification = Linear(2048, 4000)\n",
    "    \n",
    "    \n",
    "    def forward(self, x):\n",
    "        # HEAD\n",
    "        # 64x64\n",
    "        out = self.head(x)\n",
    "        # 32x32\n",
    "        # EMBEDDING\n",
    "        out = self.block1(out) + out\n",
    "        out = self.block2(out) + out\n",
    "        out = self.block3(out) + self.skipdown1(out) # 16x16\n",
    "        out = self.block4(out) + out\n",
    "        out = self.block5(out) + self.skipdown2(out)# 8x8\n",
    "        out = self.block6(out) + out\n",
    "        out = self.block7(out) + self.skipdown3(out)# 4x4\n",
    "        out = self.block8(out) + out\n",
    "        out = self.block9(out) + self.skipdown4(out)# 2x2\n",
    "        out = self.block10(out) + self.skipdown5(out)\n",
    "        \n",
    "        out = self.avgpool(out)\n",
    "        embedding = torch.flatten(out,1)\n",
    "        \n",
    "        classification = self.classification(embedding)\n",
    "                \n",
    "        return embedding, classification\n",
    "\n",
    "model = Model()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For Classification Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 2048]) torch.Size([32, 4000])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "184.49763504399195"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from time import time\n",
    "model=Model()\n",
    "data=torch.rand((32,3,64,64))\n",
    "ti=time()\n",
    "out = model.forward(data)\n",
    "print(out[0].shape, out[1].shape)\n",
    "tf=time()\n",
    "(tf-ti)*380638/(60*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "RUN_NUMBER = 26.5  # <============================= CHANGE THIS EVERY TIME ======================<<<<<<<<<\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (head): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "    (1): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (block1): BasicBlock(\n",
       "    (block): Sequential(\n",
       "      (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "      (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): ReLU()\n",
       "    )\n",
       "  )\n",
       "  (block2): BasicBlock(\n",
       "    (block): Sequential(\n",
       "      (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "      (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): ReLU()\n",
       "    )\n",
       "  )\n",
       "  (block3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (block): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU()\n",
       "        (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): ReLU()\n",
       "      )\n",
       "    )\n",
       "    (1): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (skipdown1): SkipDownSample(\n",
       "    (skip): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "  )\n",
       "  (block4): BasicBlock(\n",
       "    (block): Sequential(\n",
       "      (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "      (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): ReLU()\n",
       "    )\n",
       "  )\n",
       "  (block5): BasicBlock(\n",
       "    (block): Sequential(\n",
       "      (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "      (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): ReLU()\n",
       "    )\n",
       "  )\n",
       "  (skipdown2): SkipDownSample(\n",
       "    (skip): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  )\n",
       "  (block6): BasicBlock(\n",
       "    (block): Sequential(\n",
       "      (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "      (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): ReLU()\n",
       "    )\n",
       "  )\n",
       "  (block7): BasicBlock(\n",
       "    (block): Sequential(\n",
       "      (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "      (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): ReLU()\n",
       "    )\n",
       "  )\n",
       "  (skipdown3): SkipDownSample(\n",
       "    (skip): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "  )\n",
       "  (block8): BasicBlock(\n",
       "    (block): Sequential(\n",
       "      (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "      (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): ReLU()\n",
       "    )\n",
       "  )\n",
       "  (block9): BasicBlock(\n",
       "    (block): Sequential(\n",
       "      (0): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "      (3): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (4): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): ReLU()\n",
       "    )\n",
       "  )\n",
       "  (skipdown4): SkipDownSample(\n",
       "    (skip): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  )\n",
       "  (block10): BasicBlock(\n",
       "    (block): Sequential(\n",
       "      (0): Conv2d(1024, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "      (3): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (4): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): ReLU()\n",
       "    )\n",
       "  )\n",
       "  (skipdown5): SkipDownSample(\n",
       "    (skip): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  )\n",
       "  (avgpool): AvgPool2d(kernel_size=8, stride=1, padding=0)\n",
       "  (classification): Linear(in_features=2048, out_features=4000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch import optim\n",
    "\n",
    "# model_path = 'hw2p2_models/model_26_7/model.pt'\n",
    "# model = torch.load(model_path)\n",
    "NUM_EPOCHS = 40\n",
    "learning_rate = 1e-1\n",
    "mile_stones = [3,5,6,8,12,16,20,24,28,32,36,40] # [4,5,6,8,12,16,20,24,28,32,36,40]\n",
    "gamma = 0.2\n",
    "optimizer = optim.SGD(model.parameters(), momentum=0.9, weight_decay=5e-5, lr=learning_rate)\n",
    "# TODO: TRY RMSProp\n",
    "# optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=mile_stones, gamma=gamma)\n",
    "criterion = CrossEntropyLoss()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def validate(model, val_loader, val_dataset):\n",
    "    model.eval()\n",
    "    total = len(val_dataset)\n",
    "    num_correct = 0\n",
    "    for i, (x,y) in enumerate(val_loader):\n",
    "        x = x.to(device)\n",
    "        y = y.reshape(-1).to(device)\n",
    "\n",
    "        out = model(x)[1]\n",
    "\n",
    "        out = out.to(\"cpu\")\n",
    "        y = y.to(\"cpu\")\n",
    "\n",
    "        batch_predictions = np.argmax(out.data, axis=1)\n",
    "        num_correct += (batch_predictions == y).sum()\n",
    "    \n",
    "    accuracy = num_correct.item() / total\n",
    "        \n",
    "    # Deallocate memory in GPU\n",
    "    torch.cuda.empty_cache()\n",
    "    del x\n",
    "    del y\n",
    "    \n",
    "    model.train()\n",
    "    return accuracy\n",
    "\n",
    "def save_state(AUC, accuracy, running_max, model_number, model, train_loader_args, device, NUM_EPOCHS, learning_rate, optimizer, criterion):\n",
    "    path = './hw2p2_models/model_' + str(RUN_NUMBER) + '_'+str(model_number)\n",
    "    if not os.path.isdir(path):\n",
    "        os.makedirs(path)\n",
    "    torch.save(model, path+'/model.pt')\n",
    "    # write parameter tracking file\n",
    "    parameter_file = open(path+'/hyperparameters.txt', 'w')\n",
    "    parameter_file.write('AUC:\\n' + str(AUC))\n",
    "    parameter_file.write('Accuracy:\\n' + str(accuracy))\n",
    "    parameter_file.write('Running Max:\\n' + str(running_max[0]) + \"  \" + str(running_max[1]))\n",
    "    parameter_file.write('\\nModel:\\n' + str(model))\n",
    "    parameter_file.write('\\ntrain_loader_args:\\n' + str(train_loader_args))\n",
    "    parameter_file.write('\\nDevice:\\n' + str(device))\n",
    "    parameter_file.write('\\nNUM_EPOCHS:\\n' + str(NUM_EPOCHS))\n",
    "    parameter_file.write('\\nLearning Rate:\\n' + str(learning_rate))\n",
    "    parameter_file.write('\\nOptimizer:\\n' + str(optimizer))\n",
    "    parameter_file.write('\\nCriterion:\\n' + str(criterion))\n",
    "    parameter_file.close()\n",
    "    \n",
    "\n",
    "sub_name = './submission6.csv'\n",
    "def verify(model, data_loader, sub_name, test=False):\n",
    "    model.eval()\n",
    "    \n",
    "    cos_sim = CosineSimilarity(dim=1)\n",
    "    cosine_similarity = torch.Tensor([])\n",
    "    true_similarity = torch.Tensor([])\n",
    "    if not test:\n",
    "        for i, (x,y) in enumerate(data_loader):\n",
    "            img1 = x[0].to(device)\n",
    "            img2 = x[1].to(device)\n",
    "            y = y.to(\"cpu\")\n",
    "            \n",
    "            out1 = model(img1)[0].to(\"cpu\")\n",
    "            out2 = model(img2)[0].to(\"cpu\")\n",
    "            \n",
    "            cosine_similarity = torch.cat((cosine_similarity.detach(), cos_sim(out1,out2).detach()), 0)\n",
    "            true_similarity = torch.cat((true_similarity, y), 0)\n",
    "\n",
    "            del x, y, img1, img2, out1, out2\n",
    "            torch.cuda.empty_cache()\n",
    "        model.train()\n",
    "        try:  \n",
    "            AUC = roc_auc_score(true_similarity.type(torch.DoubleTensor), cosine_similarity.type(torch.DoubleTensor).detach().numpy())\n",
    "            return AUC\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            return -1\n",
    "    else:\n",
    "        for i, (x) in enumerate(data_loader):\n",
    "            img1 = x[0].to(device)\n",
    "            img2 = x[1].to(device)\n",
    "            \n",
    "            out1 = model(img1)[0].to(\"cpu\")\n",
    "            out2 = model(img2)[0].to(\"cpu\")\n",
    "            \n",
    "            cosine_similarity = torch.cat((cosine_similarity.detach(), cos_sim(out1,out2).detach()), 0)\n",
    "            if i%1000==0:\n",
    "                print(\"Verification\", i, end='\\r')\n",
    "        model.train()\n",
    "        return write_submission(sub_name, cosine_similarity)\n",
    "\n",
    "def write_submission(sub_name, cosine_similarity):\n",
    "    pair_path = \"verification_pairs_test.txt\"\n",
    "    pairs = open(pair_path, \"r\").readlines(-1)\n",
    "    \n",
    "    submission = csv.writer(open(sub_name, \"w\"))\n",
    "    submission.writerow(['Id','Category'])\n",
    "\n",
    "    self_length=len(pairs)\n",
    "    for i in range(self_length):\n",
    "        submission.writerow([pairs[i].strip(),cosine_similarity[i].item()])\n",
    "        if i%1000==0:\n",
    "            print(\"Saved {} predicitons\".format((i+1)*batchsize), end='\\r')\n",
    "\n",
    "    print(\"Submission File COMPLETE\")\n",
    "    return self_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.00025, 0.5239093956700484)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validate(model, val_loader, val_dataset), verify(model, ver_val_loader, sub_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tImprovement: 0.152n: 1480\n",
      "Epoch 0 Accuracy: 0.152 AUC: 0.7680813549873932\n",
      "Time for epoch: 2168.746775865555\n",
      "   Running Max: Model_26.5_0 0.7680813549873932 \n",
      "\n",
      "\tImprovement: 0.16737500000000002 | Percent Improvement: 110.11513157894737 %\n",
      "Epoch 1 Accuracy: 0.319375 AUC: 0.8650197296941574\n",
      "Time for epoch: 2193.874383211136\n",
      "   Running Max: Model_26.5_1 0.8650197296941574 \n",
      "\n",
      "\tImprovement: 0.17224999999999996 | Percent Improvement: 53.933463796477476 %\n",
      "Epoch 2 Accuracy: 0.491625 AUC: 0.8694364456097883\n",
      "Time for epoch: 2207.7084872722626\n",
      "   Running Max: Model_26.5_2 0.8694364456097883 \n",
      "\n",
      "\tImprovement: 0.08337499999999998 | Percent Improvement: 16.95906432748538 %\n",
      "Epoch 3 Accuracy: 0.575 AUC: 0.8555907165917824\n",
      "Time for epoch: 2204.429117202759\n",
      "   Running Max: Model_26.5_2 0.8694364456097883 \n",
      "\n",
      "\tImprovement: 0.16762500000000002 | Percent Improvement: 29.152173913043484 %\n",
      "Epoch 4 Accuracy: 0.742625 AUC: 0.8894602141491199\n",
      "Time for epoch: 2209.494797229767\n",
      "   Running Max: Model_26.5_4 0.8894602141491199 \n",
      "\n",
      "\tImprovement: 0.022625000000000006 | Percent Improvement: 3.0466251472816035 %\n",
      "Epoch 5 Accuracy: 0.76525 AUC: 0.9025310268473523\n",
      "Time for epoch: 2209.864368200302\n",
      "   Running Max: Model_26.5_5 0.9025310268473523 \n",
      "\n",
      "\tImprovement: -0.0022499999999999742 | Percent Improvement: -0.29402156158117926 %\n",
      "Epoch 6 Accuracy: 0.763 AUC: 0.9053365403226432\n",
      "Time for epoch: 2205.591673374176\n",
      "   Running Max: Model_26.5_6 0.9053365403226432 \n",
      "\n",
      "\tImprovement: 0.003249999999999975 | Percent Improvement: 0.42595019659239514 %\n",
      "Epoch 7 Accuracy: 0.76625 AUC: 0.9053216753078167\n",
      "Time for epoch: 2210.6921458244324\n",
      "   Running Max: Model_26.5_6 0.9053365403226432 \n",
      "\n",
      "Epoch: 8 | Iteration: 1480\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-864bb03de18a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;31m# validation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m     \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m     \u001b[0mAUC\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mverify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mver_val_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msub_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;31m#     val_accuracies.append(accuracy)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-b705c999f16e>\u001b[0m in \u001b[0;36mvalidate\u001b[0;34m(model, val_loader, val_dataset)\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# toggle to train so save gradients\n",
    "from time import time\n",
    "# val_accuracies=[]\n",
    "accuracy = 0.\n",
    "model_number = 0 \n",
    "prev_acc = 0\n",
    "running_max = ['',0.]\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    ti = time()\n",
    "    model.train()\n",
    "    for i, (x,y) in enumerate(train_loader):\n",
    "#         if i%10000==0:\n",
    "#             for p in model.parameters():\n",
    "#                 print(p.name, p.data)\n",
    "#             print(i, ':', x.shape, y.shape)\n",
    "#         print(\"Completed\", i)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        x = x.to(device)\n",
    "        y = y.reshape(-1).to(device) # need to turn to row\n",
    "\n",
    "        output = model(x)[1]\n",
    "        loss = criterion(output, y)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # progress\n",
    "        if i%10==0:\n",
    "            print('Epoch:', epoch, '| Iteration:', i, end='\\r')\n",
    "\n",
    "    \n",
    "    # Deallocate memory in GPU\n",
    "    torch.cuda.empty_cache()\n",
    "    del x\n",
    "    del y\n",
    "\n",
    "    # validation\n",
    "    accuracy = validate(model, val_loader, val_dataset)\n",
    "    AUC = verify(model, ver_val_loader, sub_name)\n",
    "#     val_accuracies.append(accuracy)\n",
    "\n",
    "    print(\"Epoch\", epoch, \"Accuracy:\", accuracy, \"AUC:\", AUC)\n",
    "    if prev_acc == 0:\n",
    "        print(\"\\tImprovement:\", accuracy-prev_acc)\n",
    "    else:\n",
    "        print(\"\\tImprovement:\", accuracy-prev_acc, \"| Percent Improvement:\", 100*(accuracy-prev_acc)/prev_acc, '%')\n",
    "    # tracking running best AUC\n",
    "    if running_max[1]<AUC:\n",
    "        running_max[0]='Model_' + str(RUN_NUMBER) + '_' + str(epoch)\n",
    "        running_max[1]=AUC\n",
    "        \n",
    "    if (AUC > 0.8):\n",
    "        save_state(AUC, accuracy, running_max, model_number, model, train_loader_args, device, NUM_EPOCHS, learning_rate, optimizer, criterion)\n",
    "    model_number+=1\n",
    "\n",
    "    scheduler.step()\n",
    "    \n",
    "    tf=time()\n",
    "    print(\"Time for epoch:\", tf-ti)\n",
    "    print('   Running Max:', *running_max,'\\n')\n",
    "\n",
    "    prev_acc = accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_state(AUC, accuracy, model_number, model, train_loader_args, device, NUM_EPOCHS, learning_rate, optimizer, criterion)\n",
    "# model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metric Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from time import time\n",
    "import torch\n",
    "from torch import optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch.nn import Module\n",
    "from torch.nn import Linear, CosineSimilarity, BatchNorm1d, BatchNorm2d, Sequential, Dropout, ReLU, LeakyReLU, Conv2d, MaxPool2d, AvgPool2d, AdaptiveAvgPool2d, CrossEntropyLoss\n",
    "import csv\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def validate(model, val_loader, val_dataset):\n",
    "    model.eval()\n",
    "    total = len(val_dataset)\n",
    "    num_correct = 0\n",
    "    for i, (x,y) in enumerate(val_loader):\n",
    "        x = x.to(device)\n",
    "        y = y.reshape(-1).to(device)\n",
    "\n",
    "        out = model(x)[1]\n",
    "\n",
    "        out = out.to(\"cpu\")\n",
    "        y = y.to(\"cpu\")\n",
    "\n",
    "        batch_predictions = np.argmax(out.data, axis=1)\n",
    "        num_correct += (batch_predictions == y).sum()\n",
    "    \n",
    "    accuracy = num_correct.item() / total\n",
    "        \n",
    "    # Deallocate memory in GPU\n",
    "    torch.cuda.empty_cache()\n",
    "    del x\n",
    "    del y\n",
    "    \n",
    "    model.train()\n",
    "    return accuracy\n",
    "\n",
    "def save_state(AUC, accuracy, running_max, model_number, model, train_loader_args, device, NUM_EPOCHS, learning_rate, optimizer, criterion):\n",
    "    path = './hw2p2_models/model_' + str(RUN_NUMBER) + '_'+str(model_number)\n",
    "    if not os.path.isdir(path):\n",
    "        os.makedirs(path)\n",
    "    torch.save(model, path+'/model.pt')\n",
    "    # write parameter tracking file\n",
    "    parameter_file = open(path+'/hyperparameters.txt', 'w')\n",
    "    parameter_file.write('AUC:\\n' + str(AUC))\n",
    "    parameter_file.write('Accuracy:\\n' + str(accuracy))\n",
    "    parameter_file.write('Running Max:\\n' + str(running_max[0]) +\" \"+ str(running_max[1]))\n",
    "    parameter_file.write('\\nModel:\\n' + str(model))\n",
    "    parameter_file.write('\\ntrain_loader_args:\\n' + str(train_loader_args))\n",
    "    parameter_file.write('\\nDevice:\\n' + str(device))\n",
    "    parameter_file.write('\\nNUM_EPOCHS:\\n' + str(NUM_EPOCHS))\n",
    "    parameter_file.write('\\nLearning Rate:\\n' + str(learning_rate))\n",
    "    parameter_file.write('\\nOptimizer:\\n' + str(optimizer))\n",
    "    parameter_file.write('\\nCriterion:\\n' + str(criterion))\n",
    "    parameter_file.close()\n",
    "    \n",
    "\n",
    "sub_name = './submission1.csv'\n",
    "def verify(model, data_loader, sub_name, test=False):\n",
    "    model.eval()\n",
    "    \n",
    "    cos_sim = CosineSimilarity(dim=1)\n",
    "    cosine_similarity = torch.Tensor([])\n",
    "    true_similarity = torch.Tensor([])\n",
    "    if not test:\n",
    "        for i, (x,y) in enumerate(data_loader):\n",
    "            img1 = x[0].to(device)\n",
    "            img2 = x[1].to(device)\n",
    "            y = y.to(\"cpu\")\n",
    "            \n",
    "            out1 = model(img1)[0].to(\"cpu\")\n",
    "            out2 = model(img2)[0].to(\"cpu\")\n",
    "            \n",
    "            cosine_similarity = torch.cat((cosine_similarity.detach(), cos_sim(out1,out2).detach()), 0)\n",
    "            true_similarity = torch.cat((true_similarity, y), 0)\n",
    "\n",
    "            del x, y, img1, img2, out1, out2\n",
    "            torch.cuda.empty_cache()\n",
    "        model.train()\n",
    "        try:  \n",
    "            AUC = roc_auc_score(true_similarity.type(torch.DoubleTensor), cosine_similarity.type(torch.DoubleTensor).detach().numpy())\n",
    "            return AUC\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            return -1\n",
    "    else:\n",
    "        for i, (x) in enumerate(data_loader):\n",
    "            img1 = x[0].to(device)\n",
    "            img2 = x[1].to(device)\n",
    "            \n",
    "            out1 = model(img1)[0].to(\"cpu\")\n",
    "            out2 = model(img2)[0].to(\"cpu\")\n",
    "            \n",
    "            cosine_similarity = torch.cat((cosine_similarity.detach(), cos_sim(out1,out2).detach()), 0)\n",
    "            if i%1000==0:\n",
    "                print(\"Verification\", i, end='\\r')\n",
    "        model.train()\n",
    "        return write_submission(sub_name, cosine_similarity)\n",
    "\n",
    "def write_submission(sub_name, cosine_similarity):\n",
    "    pair_path = \"verification_pairs_test.txt\"\n",
    "    pairs = open(pair_path, \"r\").readlines(-1)\n",
    "    \n",
    "    submission = csv.writer(open(sub_name, \"w\"))\n",
    "    submission.writerow(['Id','Category'])\n",
    "\n",
    "    self_length=len(pairs)\n",
    "    for i in range(self_length):\n",
    "        submission.writerow([pairs[i].strip(),cosine_similarity[i].item()])\n",
    "        if i%1000==0:\n",
    "            print(\"Saved {} predicitons\".format((i+1)*batchsize), end='\\r')\n",
    "\n",
    "    print(\"Submission File COMPLETE\")\n",
    "    return self_length\n",
    "\n",
    "# ------------------------------------------------ DATASETS ------------------------------------------------ #\n",
    "class TrainVerificationMyDataset(Dataset):\n",
    "    def __init__(self, train_path='classification_data/train_data/', N_generated=10000, ratio_of_same=0.3):\n",
    "        self.train_dataset = ImageFolder(root=train_path, transform=ToTensor())\n",
    "        \n",
    "        self.length_train = len(self.train_dataset)\n",
    "        self.length = N_generated\n",
    "        \n",
    "        self.by_label = {}\n",
    "        for i in range(self.length_train):\n",
    "            y = self.train_dataset[i][1]\n",
    "            if y in self.by_label:\n",
    "                self.by_label[y].append(i)\n",
    "            else:\n",
    "                self.by_label[y] = [i]\n",
    "            if i%1000==0:\n",
    "                print(\"by_label {}\".format(i), end='\\r')\n",
    "        print(\"by_label created         \")\n",
    "        \n",
    "        self.load_pairs(N_generated, ratio_of_same)\n",
    "        #         del by_label, same, different, N_generated, total_same, total_different, length_train\n",
    "        \n",
    "    def load_pairs(self, N_generated, ratio_of_same):\n",
    "        total_same = int(N_generated*(ratio_of_same))\n",
    "        total_different = N_generated - total_same\n",
    "        self.pairs = []\n",
    "        same = 0\n",
    "        different = 0\n",
    "        while different < total_different:\n",
    "            # randomly select an index\n",
    "            index1 = torch.randint(0,self.length_train, (1,)).item()\n",
    "            y1 = self.train_dataset[index1][1]\n",
    "            # randomly select another index\n",
    "            index2 = torch.randint(0,self.length_train, (1,)).item()\n",
    "            # try not to get matches - this doesn't really matter and is unlikely\n",
    "            while index2 == index1:\n",
    "                index2 = torch.randint(0,self.length_train, (1,)).item()\n",
    "\n",
    "            y2 = self.train_dataset[index2][1]\n",
    "\n",
    "            pair_label = int(y1==y2)\n",
    "\n",
    "            if pair_label == 1:\n",
    "                same += 1\n",
    "            else:\n",
    "                different += 1\n",
    "            self.pairs.append((index1, index2, y1, y2, pair_label))\n",
    "\n",
    "        while same < total_same:\n",
    "            # randomly select an index\n",
    "            index1 = torch.randint(0,self.length_train, (1,)).item()\n",
    "            y1 = self.train_dataset[index1][1]\n",
    "\n",
    "            # randomly select same labeled item\n",
    "            by_label_index = torch.randint(0,len(self.by_label[y1]), (1,)).item()\n",
    "            index2 = self.by_label[y1][by_label_index]\n",
    "\n",
    "            self.pairs.append((index1, index2, y1, y1, 1))\n",
    "            same+=1\n",
    "        print(\"Train Data Loaded: Total {} with {} matches and {} different\".format(N_generated, total_same, total_different))\n",
    "            \n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "    def __getitem__(self, index):\n",
    "        index1, index2, y1, y2, match = self.pairs[index]\n",
    "        x1 = self.train_dataset[index1][0]\n",
    "        x2 = self.train_dataset[index2][0]\n",
    "        return (x1, x2), (y1, y2), match\n",
    "\n",
    "class VerificationMyDataset(Dataset):\n",
    "    def __init__(self, pair_path, test=False):\n",
    "        pairs = open(pair_path, \"r\").readlines(-1)\n",
    "        self.test = test\n",
    "        self.x=[]\n",
    "        if not self.test:\n",
    "            self.y=[]\n",
    "        self.to_tensor = ToTensor()\n",
    "        self.length=len(pairs)\n",
    "        for i in range(self.length):\n",
    "            line = pairs[i].strip().split(' ')\n",
    "            image1 = self.to_tensor(Image.open(line[0]))\n",
    "            image2 = self.to_tensor(Image.open(line[1]))\n",
    "            self.x.append((image1,image2))\n",
    "            if not self.test:\n",
    "                self.y.append(int(line[2]))\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "    def __getitem__(self, index):\n",
    "        if not self.test:\n",
    "            return self.x[index], self.y[index]\n",
    "        else:\n",
    "            return self.x[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "by_label created         \n",
      "Train Data Loaded: Total 100000 with 50000 matches and 50000 different\n",
      "All Data Loaded\n"
     ]
    }
   ],
   "source": [
    "cuda = True\n",
    "batchsize = 256 if cuda else 64\n",
    "num_workers = 4 if cuda else 0\n",
    "\n",
    "val_dataset = ImageFolder(root='classification_data/val_data/', transform=ToTensor())\n",
    "val_loader_args = dict(batch_size=batchsize, shuffle=False, num_workers=num_workers, pin_memory=True) if cuda else dict(shuffle=False, batch_size=batchsize)\n",
    "val_loader = DataLoader(val_dataset, **val_loader_args)\n",
    "\n",
    "ver_train_dataset = TrainVerificationMyDataset(N_generated=100000, ratio_of_same=0.5)\n",
    "ver_train_loader_args = dict(batch_size=batchsize, shuffle=True, num_workers=num_workers, pin_memory=True) if cuda else dict(shuffle=True, batch_size=batchsize)\n",
    "ver_train_loader = DataLoader(ver_train_dataset, **ver_train_loader_args)\n",
    "\n",
    "pairs_path = \"verification_pairs_val.txt\"\n",
    "ver_val_dataset = VerificationMyDataset(pairs_path)\n",
    "ver_val_loader_args = dict(batch_size=batchsize, shuffle=True, num_workers=num_workers, pin_memory=True) if cuda else dict(shuffle=True, batch_size=batchsize)\n",
    "ver_val_loader = DataLoader(ver_val_dataset, **ver_val_loader_args)\n",
    "\n",
    "print(\"All Data Loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 64, 64]) torch.Size([3, 64, 64]) (211, 3210)\n",
      "0 torch.Size([256, 3, 64, 64]) torch.Size([256, 3, 64, 64]) torch.Size([256]) torch.Size([256]) torch.Size([256])\n",
      "1 torch.Size([256, 3, 64, 64]) torch.Size([256, 3, 64, 64]) torch.Size([256]) torch.Size([256]) torch.Size([256])\n",
      "2 torch.Size([256, 3, 64, 64]) torch.Size([256, 3, 64, 64]) torch.Size([256]) torch.Size([256]) torch.Size([256])\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "print(ver_train_dataset[i][0][0].shape, ver_train_dataset[i][0][1].shape, ver_train_dataset[i][1])\n",
    "for i, (x, y, match) in enumerate(ver_train_loader):\n",
    "    print(i, x[0].shape, x[1].shape, y[0].shape, y[1].shape, match.shape)\n",
    "    if i==2:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model_18_\n",
    "class BasicBlock(Module): \n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, dropout=False, drop_rate=0.3): # groups=1, dilation=1, norm_layer=None\n",
    "        super().__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        layers = [\n",
    "            Conv2d(in_channels, out_channels, kernel_size=kernel_size, padding=kernel_size-2, stride=stride, bias=False),  # , padding=kernel_size-1\n",
    "            BatchNorm2d(out_channels),\n",
    "            ReLU(),\n",
    "            Conv2d(out_channels, out_channels, kernel_size=kernel_size, padding=kernel_size-2, stride=1, bias=False),  # , padding=kernel_size-1\n",
    "            BatchNorm2d(out_channels),\n",
    "            ReLU()\n",
    "        ]\n",
    "        \n",
    "        if dropout:\n",
    "            layers.append(Dropout(drop_rate))\n",
    "            \n",
    "        self.block = Sequential(*layers)\n",
    "\n",
    "    def forward(self,x):\n",
    "        out = self.block(x)\n",
    "         \n",
    "        return out\n",
    "\n",
    "class SkipDownSample(Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1): # groups=1, dilation=1, norm_layer=None\n",
    "        super().__init__()\n",
    "    \n",
    "#     layers = [\n",
    "#         Conv2d(in_channels, out_channels, kernel_size=1, stride=1, bias=False)\n",
    "#     ]\n",
    "    \n",
    "        self.skip = Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False)\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.skip(x)\n",
    "# Model_26_ \n",
    "class Model(Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # HEAD\n",
    "        head = [\n",
    "            Conv2d(3, 64, kernel_size=3, stride=1, bias=False), # 64x64\n",
    "            MaxPool2d(kernel_size=3, stride=2, padding=1) # 32x32\n",
    "        ]\n",
    "        self.head = Sequential(*head)\n",
    "        \n",
    "        # Embedder\n",
    "        self.block1 = BasicBlock(64,64)\n",
    "        self.block2 = BasicBlock(64,64)\n",
    "        \n",
    "        self.block3 = Sequential(BasicBlock(64,128), MaxPool2d(kernel_size=3, stride=2, padding=1))#, stride=2) # 16x16\n",
    "        self.skipdown1 = SkipDownSample(64,128, stride=2)             # MAYBE SHIFT THIS DOWN 1 SET OF BLOCKS\n",
    "        self.block4 = BasicBlock(128,128)\n",
    "        \n",
    "        self.block5 = BasicBlock(128,256) # , stride=2) # 16x16\n",
    "        self.skipdown2 = SkipDownSample(128,256) # , stride=2)\n",
    "        self.block6 = BasicBlock(256,256)\n",
    "        \n",
    "        self.block7 = BasicBlock(256,512, stride=2) # 8x8\n",
    "        self.skipdown3 = SkipDownSample(256,512, stride=2)            # MAYBE SHIFT THIS DOWN 1 SET OF BLOCKS\n",
    "        self.block8 = BasicBlock(512,512)\n",
    "        \n",
    "        # I added\n",
    "        self.block9 = BasicBlock(512, 1024) # 8x8\n",
    "        self.skipdown4 = SkipDownSample(512,1024)\n",
    "        \n",
    "        self.block10 = BasicBlock(1024,2048) # 8x8\n",
    "        self.skipdown5 = SkipDownSample(1024,2048)\n",
    "\n",
    "        # EMBEDDING\n",
    "        self.avgpool = AvgPool2d(kernel_size=8, stride=1) # kernel size 4 -> 8\n",
    "        \n",
    "        # CLASSIFICATION\n",
    "        self.classification = Linear(2048, 4000)\n",
    "    \n",
    "    \n",
    "    def forward(self, x):\n",
    "        # HEAD\n",
    "        # 64x64\n",
    "        out = self.head(x)\n",
    "        # 32x32\n",
    "        # EMBEDDING\n",
    "        out = self.block1(out) + out\n",
    "        out = self.block2(out) + out\n",
    "        out = self.block3(out) + self.skipdown1(out) # 16x16\n",
    "        out = self.block4(out) + out\n",
    "        out = self.block5(out) + self.skipdown2(out)# 8x8\n",
    "        out = self.block6(out) + out\n",
    "        out = self.block7(out) + self.skipdown3(out)# 4x4\n",
    "        out = self.block8(out) + out\n",
    "        out = self.block9(out) + self.skipdown4(out)# 2x2\n",
    "        out = self.block10(out) + self.skipdown5(out)\n",
    "        \n",
    "        out = self.avgpool(out)\n",
    "        embedding = torch.flatten(out,1)\n",
    "        \n",
    "        classification = self.classification(embedding)\n",
    "                \n",
    "        return embedding, classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContrastiveLoss(Module):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        margin: \n",
    "    \"\"\"\n",
    "    def __init__(self, margin=1.):\n",
    "        super().__init__()\n",
    "        self.margin = margin\n",
    "\n",
    "    def forward(self, x1, x2, labels):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: feature matrix with shape (batch_size, feat_dim).\n",
    "            labels: ground truth labels with shape (batch_size).\n",
    "        \"\"\"\n",
    "#         batch_size = x1.shape[0]\n",
    "#         dist = torch.zeros(batch_size)\n",
    "#         for i in range(batch_size):\n",
    "#             dist[i] = torch.norm(x1[i]-x2[i])\n",
    "        \n",
    "        dist = CosineSimilarity(dim=1)(x1, x2)\n",
    "    \n",
    "        #total_loss = labels*dist + (1-labels)*(self.margin-dist)\n",
    "        total_loss = (1-labels)*dist + labels*(self.margin-dist)\n",
    "\n",
    "        loss = total_loss.mean()\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if cuda else \"cpu\")\n",
    "\n",
    "model_path = 'hw2p2_models/model_26.3_7/model.pt'\n",
    "model = torch.load(model_path).to(device)\n",
    "# model = Model().to(device)\n",
    "\n",
    "RUN_NUMBER = 26.4  # <============================= CHANGE THIS EVERY TIME ======================<<<<<<<<<\n",
    "\n",
    "NUM_EPOCHS = 15 # 40 #\n",
    "learning_rate = 1e-6 # 1e-3 #1e-1\n",
    "mile_stones = [5,10] # [5,10,15] # [4,7,10,13,16,19,22,25] #\n",
    "gamma = 0.1\n",
    "optimizer = optim.SGD(model.parameters(), momentum=0.9, weight_decay=5e-5, lr=learning_rate)\n",
    "\n",
    "# TODO: TRY RMSProp\n",
    "# optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=mile_stones, gamma=gamma)\n",
    "\n",
    "criterion_ver = ContrastiveLoss()\n",
    "criterion_class = CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.6667)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L=ContrastiveLoss(device=device)\n",
    "x1 = torch.Tensor([[1,1],[2,2],[3,3]])\n",
    "x2 = torch.Tensor([[2,2],[4,4],[6,6]])\n",
    "y=torch.Tensor([1,0,1])\n",
    "L(x1,x2,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation AUC: 0.9254600541437519\n"
     ]
    }
   ],
   "source": [
    "AUC = verify(model, ver_val_loader, 'none')\n",
    "print(\"Validation AUC:\", AUC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Accuracy: 0.75625 AUC: 0.9239550230072234 Loss: 0.24474437534809113\n",
      "\tImprovement: 0.9239550230072234\n",
      "Train Data Loaded: Total 100000 with 50000 matches and 50000 different\n",
      "Time for epoch: 1187.6698563098907\n",
      "   Running Max: Model_26.4_0 0.9239550230072234 \n",
      "\n",
      "Epoch: 1 | Iteration: 10\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-73470ed32a31>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mimg1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0mimg2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m#         y = y.reshape(-1).to(device) # need to turn to row\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# toggle to train so save gradients\n",
    "# running_max = ['',0.]\n",
    "# while running_max[1] < 0.98:\n",
    "\n",
    "val_accuracies=[]\n",
    "model_number=0\n",
    "prev_auc = 0\n",
    "running_max = ['',0.]\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    ti = time()\n",
    "    model.train()\n",
    "    for i, (x, y, match) in enumerate(ver_train_loader):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        img1 = x[0].to(device)\n",
    "        img2 = x[1].to(device)\n",
    "#         y = y.reshape(-1).to(device) # need to turn to row\n",
    "\n",
    "        embedding1, out1 = model(img1)\n",
    "        embedding2, out2 = model(img2)\n",
    "        \n",
    "        loss = criterion_ver(embedding1, embedding2, match.to(device))# + criterion_class(out1, y[0].to(device)) + criterion_class(out2, y[1].to(device))# + \n",
    "        loss.backward()\n",
    "        \n",
    "#         loss = criterion_class(out1, y[0])\n",
    "#         loss.backward()\n",
    "#         loss = criterion_class(out2, y[1])\n",
    "#         loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        # progress\n",
    "        if i%10==0:\n",
    "            print('Epoch:', epoch, '| Iteration:', i, end='\\r')\n",
    "\n",
    "    \n",
    "    # Deallocate memory in GPU\n",
    "    torch.cuda.empty_cache()\n",
    "    del x\n",
    "    del y\n",
    "\n",
    "    # validation\n",
    "    accuracy = validate(model, val_loader, val_dataset)\n",
    "    AUC = verify(model, ver_val_loader, sub_name)\n",
    "    val_accuracies.append(accuracy)\n",
    "    \n",
    "    print(\"Epoch\", epoch, \"Accuracy:\", accuracy, \"AUC:\", AUC, \"Loss:\", loss.item())\n",
    "    if prev_auc == 0:\n",
    "        print(\"\\tImprovement:\", AUC-prev_auc)\n",
    "    else:\n",
    "        print(\"\\tImprovement:\", AUC-prev_auc, \"| Percent Improvement (times):\", 100*AUC/prev_auc, '%')\n",
    "    # tracking running best AUC\n",
    "    if running_max[1]<AUC:\n",
    "        running_max[0]='Model_' + str(RUN_NUMBER) + '_' + str(epoch)\n",
    "        running_max[1]=AUC\n",
    "    \n",
    "    if (accuracy > 0.65 or AUC > 0.90):\n",
    "        save_state(AUC, accuracy, running_max, model_number, model, ver_val_loader_args, device, NUM_EPOCHS, learning_rate, optimizer, criterion_class)\n",
    "    model_number+=1\n",
    "\n",
    "    scheduler.step()\n",
    "        \n",
    "    ver_train_dataset.load_pairs(100000, 0.5)\n",
    "    \n",
    "    tf=time()\n",
    "    print(\"Time for epoch:\", tf-ti)\n",
    "    print('   Running Max:', *running_max,'\\n')\n",
    "\n",
    "    prev_auc = AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.69275"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save_state(AUC, accuracy, running_max, model_number, model, ver_val_loader_args, device, NUM_EPOCHS, learning_rate, optimizer, criterion_class)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch.nn import Module, CosineSimilarity, Linear, BatchNorm1d, BatchNorm2d, Sequential, Dropout, ReLU, LeakyReLU, Conv2d, MaxPool2d, AvgPool2d, AdaptiveAvgPool2d, CrossEntropyLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VerificationMyDataset(Dataset):\n",
    "    def __init__(self, pair_path, test=False):\n",
    "        pairs = open(pair_path, \"r\").readlines(-1)\n",
    "        self.test = test\n",
    "        self.x=[]\n",
    "        if not self.test:\n",
    "            self.y=[]\n",
    "        self.to_tensor = ToTensor()\n",
    "        self.length=len(pairs)\n",
    "        for i in range(self.length):\n",
    "            line = pairs[i].strip().split(' ')\n",
    "            image1 = self.to_tensor(Image.open(line[0]))\n",
    "            image2 = self.to_tensor(Image.open(line[1]))\n",
    "            self.x.append((image1,image2))\n",
    "            if not self.test:\n",
    "                self.y.append(int(line[2]))\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    # keep this simple/quick because run many times\n",
    "    def __getitem__(self, index):\n",
    "        if not self.test:\n",
    "            return self.x[index], self.y[index]\n",
    "        else:\n",
    "            return self.x[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda = True\n",
    "batchsize = 128 if cuda else 64\n",
    "num_workers = 4 if cuda else 0\n",
    "device = torch.device(\"cuda\" if cuda else \"cpu\")\n",
    "# from Recitation 5\n",
    "pairs_path = \"verification_pairs_val.txt\"\n",
    "\n",
    "ver_val_dataset = VerificationMyDataset(pairs_path)\n",
    "ver_val_loader_args = dict(batch_size=batchsize, shuffle=False, num_workers=num_workers, pin_memory=True) if cuda else dict(shuffle=False, batch_size=batchsize)\n",
    "ver_val_loader = DataLoader(ver_val_dataset, **ver_val_loader_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model(\n",
      "  (head): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
      "    (1): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (block1): BasicBlock(\n",
      "    (block): Sequential(\n",
      "      (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU()\n",
      "      (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (5): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (block2): BasicBlock(\n",
      "    (block): Sequential(\n",
      "      (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU()\n",
      "      (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (5): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (block3): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (block): Sequential(\n",
      "        (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU()\n",
      "        (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (5): ReLU()\n",
      "      )\n",
      "    )\n",
      "    (1): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (skipdown1): SkipDownSample(\n",
      "    (skip): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "  )\n",
      "  (block4): BasicBlock(\n",
      "    (block): Sequential(\n",
      "      (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU()\n",
      "      (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (5): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (block5): BasicBlock(\n",
      "    (block): Sequential(\n",
      "      (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU()\n",
      "      (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (5): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (skipdown2): SkipDownSample(\n",
      "    (skip): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "  )\n",
      "  (block6): BasicBlock(\n",
      "    (block): Sequential(\n",
      "      (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU()\n",
      "      (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (5): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (block7): BasicBlock(\n",
      "    (block): Sequential(\n",
      "      (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU()\n",
      "      (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (5): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (skipdown3): SkipDownSample(\n",
      "    (skip): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "  )\n",
      "  (block8): BasicBlock(\n",
      "    (block): Sequential(\n",
      "      (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU()\n",
      "      (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (5): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (block9): BasicBlock(\n",
      "    (block): Sequential(\n",
      "      (0): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU()\n",
      "      (3): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (4): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (5): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (skipdown4): SkipDownSample(\n",
      "    (skip): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "  )\n",
      "  (block10): BasicBlock(\n",
      "    (block): Sequential(\n",
      "      (0): Conv2d(1024, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU()\n",
      "      (3): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (4): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (5): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (skipdown5): SkipDownSample(\n",
      "    (skip): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "  )\n",
      "  (avgpool): AvgPool2d(kernel_size=8, stride=1, padding=0)\n",
      "  (classification): Linear(in_features=2048, out_features=4000, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# RESNET BASIC BLOCK\n",
    "class BasicBlock(Module): \n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, dropout=False, drop_rate=0.3): # groups=1, dilation=1, norm_layer=None\n",
    "        super().__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        layers = [\n",
    "            Conv2d(in_channels, out_channels, kernel_size=kernel_size, padding=kernel_size-2, stride=stride, bias=False),  # , padding=kernel_size-1\n",
    "            BatchNorm2d(out_channels),\n",
    "            ReLU(),\n",
    "            Conv2d(out_channels, out_channels, kernel_size=kernel_size, padding=kernel_size-2, stride=1, bias=False),  # , padding=kernel_size-1\n",
    "            BatchNorm2d(out_channels),\n",
    "            ReLU()\n",
    "        ]\n",
    "        \n",
    "        if dropout:\n",
    "            layers.append(Dropout(drop_rate))\n",
    "            \n",
    "        self.block = Sequential(*layers)\n",
    "\n",
    "    def forward(self,x):\n",
    "        out = self.block(x)\n",
    "         \n",
    "        return out\n",
    "\n",
    "class SkipDownSample(Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1): # groups=1, dilation=1, norm_layer=None\n",
    "        super().__init__()\n",
    "    \n",
    "        self.skip = Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False)\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.skip(x)\n",
    "    \n",
    "# Model_26_ \n",
    "class Model(Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # HEAD\n",
    "        head = [\n",
    "            Conv2d(3, 64, kernel_size=3, stride=1, bias=False), # 64x64\n",
    "            MaxPool2d(kernel_size=3, stride=2, padding=1) # 32x32\n",
    "        ]\n",
    "        self.head = Sequential(*head)\n",
    "        \n",
    "        # Embedder\n",
    "        self.block1 = BasicBlock(64,64)\n",
    "        self.block2 = BasicBlock(64,64)\n",
    "        \n",
    "        self.block3 = Sequential(BasicBlock(64,128), MaxPool2d(kernel_size=3, stride=2, padding=1))#, stride=2) # 16x16\n",
    "        self.skipdown1 = SkipDownSample(64,128, stride=2)             # MAYBE SHIFT THIS DOWN 1 SET OF BLOCKS\n",
    "        self.block4 = BasicBlock(128,128)\n",
    "        \n",
    "        self.block5 = BasicBlock(128,256) # , stride=2) # 16x16\n",
    "        self.skipdown2 = SkipDownSample(128,256) # , stride=2)\n",
    "        self.block6 = BasicBlock(256,256)\n",
    "        \n",
    "        self.block7 = BasicBlock(256,512, stride=2) # 8x8\n",
    "        self.skipdown3 = SkipDownSample(256,512, stride=2)            # MAYBE SHIFT THIS DOWN 1 SET OF BLOCKS\n",
    "        self.block8 = BasicBlock(512,512)\n",
    "        \n",
    "        # I added\n",
    "        self.block9 = BasicBlock(512, 1024) # 8x8\n",
    "        self.skipdown4 = SkipDownSample(512,1024)\n",
    "        \n",
    "        self.block10 = BasicBlock(1024,2048) # 8x8\n",
    "        self.skipdown5 = SkipDownSample(1024,2048)\n",
    "\n",
    "        # EMBEDDING\n",
    "        self.avgpool = AvgPool2d(kernel_size=8, stride=1) # kernel size 4 -> 8\n",
    "        \n",
    "        # CLASSIFICATION\n",
    "        self.classification = Linear(2048, 4000)\n",
    "    \n",
    "    \n",
    "    def forward(self, x):\n",
    "        # HEAD\n",
    "        # 64x64\n",
    "        out = self.head(x)\n",
    "        # 32x32\n",
    "        # EMBEDDING\n",
    "        out = self.block1(out) + out\n",
    "        out = self.block2(out) + out\n",
    "        out = self.block3(out) + self.skipdown1(out) # 16x16\n",
    "        out = self.block4(out) + out\n",
    "        out = self.block5(out) + self.skipdown2(out)# 8x8\n",
    "        out = self.block6(out) + out\n",
    "        out = self.block7(out) + self.skipdown3(out)# 4x4\n",
    "        out = self.block8(out) + out\n",
    "        out = self.block9(out) + self.skipdown4(out)# 2x2\n",
    "        out = self.block10(out) + self.skipdown5(out)\n",
    "        \n",
    "        out = self.avgpool(out)\n",
    "        embedding = torch.flatten(out,1)\n",
    "        \n",
    "        classification = self.classification(embedding)\n",
    "                \n",
    "        return embedding#, classification\n",
    "\n",
    "model_path = 'hw2p2_models/model_26.3_7/model.pt' # 25_ for submission 4\n",
    "model = torch.load(model_path).to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation AUC: 0.925460338024243380\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def verify(model, data_loader, sub_name, test=False):\n",
    "    model.eval()\n",
    "    \n",
    "    cos_sim = CosineSimilarity(dim=1)\n",
    "    cosine_similarity = torch.Tensor([])\n",
    "    true_similarity = torch.Tensor([])\n",
    "    if not test:\n",
    "        for i, (x,y) in enumerate(data_loader):\n",
    "            img1 = x[0].to(device)\n",
    "            img2 = x[1].to(device)\n",
    "            y = y.to(\"cpu\")\n",
    "            \n",
    "            out1 = model(img1).to(\"cpu\")\n",
    "            out2 = model(img2).to(\"cpu\")\n",
    "            \n",
    "            cosine_similarity = torch.cat((cosine_similarity.detach(), cos_sim(out1,out2).detach()), 0)\n",
    "            true_similarity = torch.cat((true_similarity, y), 0)\n",
    "\n",
    "            del x, y, img1, img2, out1, out2\n",
    "            torch.cuda.empty_cache()\n",
    "            if i%10==0:\n",
    "                print(\"Verification on validation set:\", i*batchsize, end='\\r')\n",
    "            \n",
    "        AUC = roc_auc_score(true_similarity, cosine_similarity.detach().numpy())\n",
    "        return AUC\n",
    "    else:\n",
    "        for i, (x) in enumerate(data_loader):\n",
    "            img1 = x[0].to(device)\n",
    "            img2 = x[1].to(device)\n",
    "            \n",
    "            out1 = model(img1).to(\"cpu\")\n",
    "            out2 = model(img2).to(\"cpu\")\n",
    "            \n",
    "            cosine_similarity = torch.cat((cosine_similarity.detach(), cos_sim(out1,out2).detach()), 0)\n",
    "            if i%10==0:\n",
    "                print(\"Verification on test set:\", i*batchsize, end='\\r')\n",
    "        return write_submission(sub_name, cosine_similarity)\n",
    "\n",
    "def write_submission(sub_name, cosine_similarity):\n",
    "    pair_path = \"verification_pairs_test.txt\"\n",
    "    pairs = open(pair_path, \"r\").readlines(-1)\n",
    "    \n",
    "    submission = csv.writer(open(sub_name, \"w\"))\n",
    "    submission.writerow(['Id','Category'])\n",
    "\n",
    "    self_length=len(pairs)\n",
    "    for i in range(self_length):\n",
    "        submission.writerow([pairs[i].strip(),cosine_similarity[i].item()])\n",
    "        if i%10==0:\n",
    "            print(\"Saved {} predicitons\".format((i+1)*batchsize), end='\\r')\n",
    "\n",
    "    print(\"--- Submission File COMPLETE ---\")\n",
    "    return self_length\n",
    "\n",
    "# sub_name = \"DELETEME\"\n",
    "# AUC = verify(model, ver_val_loader, sub_name)\n",
    "# print(\"Validation AUC:\", AUC)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Loaded\n",
      "--- Submission File COMPLETE ---\n",
      "51835 Predictions COMPLETE\n"
     ]
    }
   ],
   "source": [
    "sub_name = './submission6.csv'\n",
    "\n",
    "pairs_path = \"verification_pairs_test.txt\"\n",
    "\n",
    "test_dataset = VerificationMyDataset(pairs_path, test=True)\n",
    "test_loader_args = dict(batch_size=batchsize, shuffle=False, num_workers=num_workers, pin_memory=True) if cuda else dict(shuffle=False, batch_size=batchsize)\n",
    "test_loader = DataLoader(test_dataset, **test_loader_args)\n",
    "print(\"Data Loaded\")\n",
    "\n",
    "T = verify(model, test_loader, sub_name, test=True)\n",
    "print(\"{} Predictions COMPLETE\".format(T))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Beginnings of Success ---------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RESNET BASIC BLOCK\n",
    "# Model_27_ \n",
    "class Model(Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # HEAD\n",
    "        head = [\n",
    "            Conv2d(3, 64, kernel_size=3, stride=1, bias=False), # 64x64\n",
    "            MaxPool2d(kernel_size=3, stride=2, padding=1) # 32x32\n",
    "        ]\n",
    "        self.head = Sequential(*head)\n",
    "        \n",
    "        # Embedder\n",
    "        self.block1 = BasicBlock(64,64)\n",
    "        self.block2 = BasicBlock(64,64)\n",
    "        \n",
    "        self.block3 = Sequential(BasicBlock(64,128), MaxPool2d(kernel_size=3, stride=2, padding=1)) #, stride=2) # 16x16\n",
    "        self.skipdown1 = SkipDownSample(64,128, stride=2)             # MAYBE SHIFT THIS DOWN 1 SET OF BLOCKS\n",
    "        self.block4 = BasicBlock(128,128)\n",
    "        \n",
    "        self.block5 = BasicBlock(128,256) # , stride=2) # 16x16\n",
    "        self.skipdown2 = SkipDownSample(128,256) # , stride=2)\n",
    "        self.block6 = BasicBlock(256,256)\n",
    "        \n",
    "        self.block7 = Sequential(BasicBlock(256,512), MaxPool2d(kernel_size=3, stride=2, padding=1)) # , stride=2) # 8x8             # MAYBE MAKE THIS MAX POOL\n",
    "        self.skipdown3 = SkipDownSample(256,512, stride=2)\n",
    "        self.block8 = BasicBlock(512,512)\n",
    "        \n",
    "        # I added\n",
    "        self.block9 = BasicBlock(512, 1024) # 8x8\n",
    "        self.skipdown4 = SkipDownSample(512,1024)\n",
    "        \n",
    "        self.block10 = BasicBlock(1024,2048) # 8x8\n",
    "        self.skipdown5 = SkipDownSample(1024,2048)\n",
    "\n",
    "        # EMBEDDING\n",
    "        self.avgpool = AvgPool2d(kernel_size=8, stride=1) # kernel size 4 -> 8\n",
    "        \n",
    "        # CLASSIFICATION\n",
    "        self.classification = Linear(2048, 4000)\n",
    "        #             Linear(1024, 4000)\n",
    "\n",
    "        \n",
    "#         ]\n",
    "#         self.classification = Sequential(*classification)\n",
    "    \n",
    "    \n",
    "    def forward(self, x):\n",
    "        # HEAD\n",
    "        # 64x64\n",
    "        out = self.head(x)\n",
    "        # 32x32\n",
    "        # EMBEDDING\n",
    "        out = self.block1(out) + out\n",
    "        out = self.block2(out) + out\n",
    "        out = self.block3(out) + self.skipdown1(out) # 16x16\n",
    "        out = self.block4(out) + out\n",
    "        out = self.block5(out) + self.skipdown2(out)# 8x8\n",
    "        out = self.block6(out) + out\n",
    "        out = self.block7(out) + self.skipdown3(out)# 4x4\n",
    "        out = self.block8(out) + out\n",
    "        out = self.block9(out) + self.skipdown4(out)# 2x2\n",
    "        out = self.block10(out) + self.skipdown5(out)\n",
    "        \n",
    "        out = self.avgpool(out)\n",
    "        embedding = torch.flatten(out,1)\n",
    "        \n",
    "        classification = self.classification(embedding)\n",
    "                \n",
    "        return embedding, classification\n",
    "\n",
    "# Model_26_ \n",
    "class Model(Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # HEAD\n",
    "        head = [\n",
    "            Conv2d(3, 64, kernel_size=3, stride=1, bias=False), # 64x64\n",
    "            MaxPool2d(kernel_size=3, stride=2, padding=1) # 32x32\n",
    "        ]\n",
    "        self.head = Sequential(*head)\n",
    "        \n",
    "        # Embedder\n",
    "        self.block1 = BasicBlock(64,64)\n",
    "        self.block2 = BasicBlock(64,64)\n",
    "        \n",
    "        self.block3 = Sequential(BasicBlock(64,128), MaxPool2d(kernel_size=3, stride=2, padding=1))#, stride=2) # 16x16\n",
    "        self.skipdown1 = SkipDownSample(64,128, stride=2)             # MAYBE SHIFT THIS DOWN 1 SET OF BLOCKS\n",
    "        self.block4 = BasicBlock(128,128)\n",
    "        \n",
    "        self.block5 = BasicBlock(128,256) # , stride=2) # 16x16\n",
    "        self.skipdown2 = SkipDownSample(128,256) # , stride=2)\n",
    "        self.block6 = BasicBlock(256,256)\n",
    "        \n",
    "        self.block7 = BasicBlock(256,512, stride=2) # 8x8\n",
    "        self.skipdown3 = SkipDownSample(256,512, stride=2)            # MAYBE SHIFT THIS DOWN 1 SET OF BLOCKS\n",
    "        self.block8 = BasicBlock(512,512)\n",
    "        \n",
    "        # I added\n",
    "        self.block9 = BasicBlock(512, 1024) # 8x8\n",
    "        self.skipdown4 = SkipDownSample(512,1024)\n",
    "        \n",
    "        self.block10 = BasicBlock(1024,2048) # 8x8\n",
    "        self.skipdown5 = SkipDownSample(1024,2048)\n",
    "\n",
    "        # EMBEDDING\n",
    "        self.avgpool = AvgPool2d(kernel_size=8, stride=1) # kernel size 4 -> 8\n",
    "        \n",
    "        # CLASSIFICATION\n",
    "        self.classification = Linear(2048, 4000)\n",
    "    \n",
    "    \n",
    "    def forward(self, x):\n",
    "        # HEAD\n",
    "        # 64x64\n",
    "        out = self.head(x)\n",
    "        # 32x32\n",
    "        # EMBEDDING\n",
    "        out = self.block1(out) + out\n",
    "        out = self.block2(out) + out\n",
    "        out = self.block3(out) + self.skipdown1(out) # 16x16\n",
    "        out = self.block4(out) + out\n",
    "        out = self.block5(out) + self.skipdown2(out)# 8x8\n",
    "        out = self.block6(out) + out\n",
    "        out = self.block7(out) + self.skipdown3(out)# 4x4\n",
    "        out = self.block8(out) + out\n",
    "        out = self.block9(out) + self.skipdown4(out)# 2x2\n",
    "        out = self.block10(out) + self.skipdown5(out)\n",
    "        \n",
    "        out = self.avgpool(out)\n",
    "        embedding = torch.flatten(out,1)\n",
    "        \n",
    "        classification = self.classification(embedding)\n",
    "                \n",
    "        return embedding, classification\n",
    "    \n",
    "# Model_25_ \n",
    "class Model(Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # HEAD\n",
    "        head = [\n",
    "            Conv2d(3, 64, kernel_size=3, stride=1, bias=False), # 64x64\n",
    "            MaxPool2d(kernel_size=3, stride=2, padding=1) # 32x32\n",
    "        ]\n",
    "        self.head = Sequential(*head)\n",
    "        \n",
    "        # Embedder\n",
    "        self.block1 = BasicBlock(64,64)\n",
    "        self.block2 = BasicBlock(64,64)\n",
    "        \n",
    "        self.block3 = BasicBlock(64,128) # , stride=2) # 32x32\n",
    "        self.skipdown1 = SkipDownSample(64,128) # , stride=2)             # MAYBE SHIFT THIS DOWN 1 SET OF BLOCKS\n",
    "        self.block4 = BasicBlock(128,128)\n",
    "        \n",
    "        self.block5 = BasicBlock(128,256, stride=2) # 16x16\n",
    "        self.skipdown2 = SkipDownSample(128,256, stride=2)\n",
    "        self.block6 = BasicBlock(256,256)\n",
    "        \n",
    "        self.block7 = BasicBlock(256,512, stride=2) # 8x8\n",
    "        self.skipdown3 = SkipDownSample(256,512, stride=2)            # MAYBE SHIFT THIS DOWN 1 SET OF BLOCKS\n",
    "        self.block8 = BasicBlock(512,512)\n",
    "        \n",
    "        # I added\n",
    "        self.block9 = BasicBlock(512, 1024) # 8x8\n",
    "        self.skipdown4 = SkipDownSample(512,1024)\n",
    "        \n",
    "        self.block10 = BasicBlock(1024, 2048) # 8x8\n",
    "        self.skipdown5 = SkipDownSample(1024,2048)\n",
    "\n",
    "        # EMBEDDING\n",
    "        self.avgpool = AvgPool2d(kernel_size=8, stride=1) # kernel size 4 -> 8\n",
    "        \n",
    "        # CLASSIFICATION\n",
    "        self.classification = Linear(2048, 4000)\n",
    "        #             Linear(1024, 4000)\n",
    "\n",
    "        \n",
    "#         ]\n",
    "#         self.classification = Sequential(*classification)\n",
    "    \n",
    "    \n",
    "    def forward(self, x):\n",
    "        # HEAD\n",
    "        # 64x64\n",
    "        out = self.head(x)\n",
    "        # 32x32\n",
    "        # EMBEDDING\n",
    "        out = self.block1(out) + out\n",
    "        out = self.block2(out) + out\n",
    "        out = self.block3(out) + self.skipdown1(out) # 16x16\n",
    "        out = self.block4(out) + out\n",
    "        out = self.block5(out) + self.skipdown2(out)# 8x8\n",
    "        out = self.block6(out) + out\n",
    "        out = self.block7(out) + self.skipdown3(out)# 4x4\n",
    "        out = self.block8(out) + out\n",
    "        out = self.block9(out) + self.skipdown4(out)# 2x2\n",
    "        out = self.block10(out) + self.skipdown5(out)\n",
    "        \n",
    "        out = self.avgpool(out)\n",
    "        embedding = torch.flatten(out,1)\n",
    "        \n",
    "        classification = self.classification(embedding)\n",
    "                \n",
    "        return embedding, classification\n",
    "    \n",
    "    \n",
    "# Model_24_ \n",
    "class Model(Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # HEAD\n",
    "        head = [\n",
    "            Conv2d(3, 64, kernel_size=3, stride=1, bias=False), # 64x64\n",
    "            MaxPool2d(kernel_size=3, stride=2, padding=1) # 32x32\n",
    "        ]\n",
    "        self.head = Sequential(*head)\n",
    "        \n",
    "        # Embedder\n",
    "        self.block1 = BasicBlock(64,64)\n",
    "        self.block2 = BasicBlock(64,64)\n",
    "        \n",
    "        self.block3 = BasicBlock(64,128, stride=2) # 16x16\n",
    "        self.skipdown1 = SkipDownSample(64,128, stride=2)             # MAYBE SHIFT THIS DOWN 1 SET OF BLOCKS\n",
    "        self.block4 = BasicBlock(128,128)\n",
    "        \n",
    "        self.block5 = BasicBlock(128,256) # , stride=2) # 16x16\n",
    "        self.skipdown2 = SkipDownSample(128,256) # , stride=2)\n",
    "        self.block6 = BasicBlock(256,256)\n",
    "        \n",
    "        self.block7 = BasicBlock(256,512, stride=2) # 8x8\n",
    "        self.skipdown3 = SkipDownSample(256,512, stride=2)            # MAYBE SHIFT THIS DOWN 1 SET OF BLOCKS\n",
    "        self.block8 = BasicBlock(512,512)\n",
    "        \n",
    "        # I added\n",
    "        self.block9 = BasicBlock(512, 1024) # 8x8\n",
    "        self.skipdown4 = SkipDownSample(512,1024)\n",
    "        \n",
    "        self.block10 = BasicBlock(1024, 2048) # 8x8\n",
    "        self.skipdown5 = SkipDownSample(1024,2048)\n",
    "\n",
    "        # EMBEDDING\n",
    "        self.avgpool = AvgPool2d(kernel_size=8, stride=1) # kernel size 4 -> 8\n",
    "        \n",
    "        # CLASSIFICATION\n",
    "        self.classification = Linear(2048, 4000)\n",
    "        #             Linear(1024, 4000)\n",
    "\n",
    "        \n",
    "#         ]\n",
    "#         self.classification = Sequential(*classification)\n",
    "    \n",
    "    \n",
    "    def forward(self, x):\n",
    "        # HEAD\n",
    "        # 64x64\n",
    "        out = self.head(x)\n",
    "        # 32x32\n",
    "        # EMBEDDING\n",
    "        out = self.block1(out) + out\n",
    "        out = self.block2(out) + out\n",
    "        out = self.block3(out) + self.skipdown1(out) # 16x16\n",
    "        out = self.block4(out) + out\n",
    "        out = self.block5(out) + self.skipdown2(out)# 8x8\n",
    "        out = self.block6(out) + out\n",
    "        out = self.block7(out) + self.skipdown3(out)# 4x4\n",
    "        out = self.block8(out) + out\n",
    "        out = self.block9(out) + self.skipdown4(out)# 2x2\n",
    "        out = self.block10(out) + self.skipdown5(out)\n",
    "        \n",
    "        out = self.avgpool(out)\n",
    "        embedding = torch.flatten(out,1)\n",
    "        \n",
    "        classification = self.classification(embedding)\n",
    "                \n",
    "        return embedding, classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model_2_\n",
    "class BasicBlock(Module): \n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, dropout=False, drop_rate=0.3): # groups=1, dilation=1, norm_layer=None\n",
    "        super().__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        layers = [\n",
    "            Conv2d(in_channels, out_channels, kernel_size=kernel_size, padding=kernel_size-2, stride=stride, bias=False),  # , padding=kernel_size-1\n",
    "            BatchNorm2d(out_channels),\n",
    "            ReLU(),\n",
    "        ]\n",
    "\n",
    "        if dropout:\n",
    "            layers.append(Dropout(drop_rate))\n",
    "            \n",
    "        self.block = Sequential(*layers)\n",
    "        \n",
    "#         # Add padding to down sample\n",
    "#         skip_layers = [\n",
    "#             Conv2d(in_channels, out_channels, kernel_size=1, stride=1, bias=False),\n",
    "#             BatchNorm2d(out_channels)\n",
    "#         ]\n",
    "#         self.skip = Sequential(*skip_layers)\n",
    "        \n",
    "#         self.relu = ReLU()\n",
    "\n",
    "    def forward(self,x):\n",
    "#         identity = x\n",
    "        \n",
    "        out = self.block(x)\n",
    "         \n",
    "#         identity = self.skip(identity)\n",
    "#         out += identity\n",
    "\n",
    "#         return self.relu(out)\n",
    "        return out\n",
    "# based on baseline\n",
    "class Model(Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "#         BigBlock = []\n",
    "#         for i in range(5):\n",
    "#             BigBlock.append(DepthWiseBlock(512, 512))\n",
    "#             BigBlock.append(BasicBlock(512, 512))\n",
    "        \n",
    "        # Embedder\n",
    "        layers = [\n",
    "            BasicBlock(3, 64, dropout=True),\n",
    "            BasicBlock(64, 64),\n",
    "            MaxPool2d(2,2),\n",
    "            BasicBlock(64, 64),\n",
    "            MaxPool2d(2,2),\n",
    "            BasicBlock(64, 128, dropout=True),\n",
    "            BasicBlock(128, 128),\n",
    "            MaxPool2d(2,2),\n",
    "            BasicBlock(128, 256, dropout=True),\n",
    "            BasicBlock(256, 256),\n",
    "            MaxPool2d(2,2),\n",
    "            BasicBlock(256, 256),\n",
    "            MaxPool2d(2,2),\n",
    "            BasicBlock(256, 256),\n",
    "            MaxPool2d(2,2)\n",
    "        ]\n",
    "        \n",
    "        self.layers = Sequential(*layers)\n",
    "        \n",
    "        # EMBEDDING\n",
    "        self.embed = Linear(256, 1024)\n",
    "        \n",
    "        # CLASSIFICATION\n",
    "        classification = [\n",
    "#             Linear(1024, 1024),          # TODO: Is this okay?\n",
    "#             BatchNorm1d(1024),\n",
    "#             ReLU(),\n",
    "            Linear(1024, 4000)\n",
    "        ]\n",
    "        self.classification = Sequential(*classification)\n",
    "    \n",
    "    \n",
    "    def forward(self, x):\n",
    "        # HEAD\n",
    "#         print(x.shape)\n",
    "#         out = self.head(x)\n",
    "#         print(out.shape)\n",
    "        # LAYERS\n",
    "        out = self.layers(x)\n",
    "        \n",
    "        # EMBEDDING\n",
    "#         print(out.shape)\n",
    "        out = torch.flatten(out,1)\n",
    "        embedding = self.embed(out)\n",
    "        \n",
    "        classification = self.classification(embedding)\n",
    "        \n",
    "        return embedding, classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model_3_, Model_7_, Model_8_, Model_9_, Model_10_\n",
    "class BasicBlock(Module): \n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, dropout=False, drop_rate=0.3): # groups=1, dilation=1, norm_layer=None\n",
    "        super().__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        layers = [\n",
    "            Conv2d(in_channels, out_channels, kernel_size=kernel_size, padding=kernel_size-2, stride=stride, bias=False),  # , padding=kernel_size-1\n",
    "            BatchNorm2d(out_channels),\n",
    "            ReLU()\n",
    "        ]\n",
    "        \n",
    "        if dropout:\n",
    "            layers.append(Dropout(drop_rate))\n",
    "            \n",
    "        self.block = Sequential(*layers)\n",
    "\n",
    "    def forward(self,x):\n",
    "        out = self.block(x)\n",
    "         \n",
    "        return out\n",
    "class Model(Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # Embedder\n",
    "        layers = [\n",
    "            BasicBlock(3, 64, dropout=True),\n",
    "            BasicBlock(64, 64),\n",
    "            MaxPool2d(2,2),\n",
    "            BasicBlock(64, 64),\n",
    "            MaxPool2d(2,2),\n",
    "            BasicBlock(64, 128, dropout=True),\n",
    "            BasicBlock(128, 128),\n",
    "            MaxPool2d(2,2),\n",
    "            BasicBlock(128, 256, dropout=True),\n",
    "            BasicBlock(256, 256),\n",
    "            MaxPool2d(2,2),\n",
    "            BasicBlock(256, 512),\n",
    "            MaxPool2d(2,2),\n",
    "            BasicBlock(512, 1024),\n",
    "            MaxPool2d(2,2)\n",
    "        ]\n",
    "        \n",
    "        # EMBEDDING\n",
    "        self.embed = Sequential(*layers)\n",
    "#         self.embed = Linear(256, 1000)\n",
    "\n",
    "        \n",
    "        # CLASSIFICATION\n",
    "        self.classification = Linear(1024, 4000)\n",
    "        #             Linear(1024, 4000)\n",
    "\n",
    "        \n",
    "#         ]\n",
    "#         self.classification = Sequential(*classification)\n",
    "    \n",
    "    \n",
    "    def forward(self, x):\n",
    "        # HEAD\n",
    "#         print(x.shape)\n",
    "#         out = self.head(x)\n",
    "#         print(out.shape)\n",
    "\n",
    "        # EMBEDDING\n",
    "        out = self.embed(x)\n",
    "        embedding = torch.flatten(out,1)\n",
    "\n",
    "#         print(embedding.shape)\n",
    "    \n",
    "        classification = self.classification(embedding)\n",
    "                \n",
    "        return embedding, classification\n",
    "model = Model()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model_4_\n",
    "class BasicBlock(Module): \n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, dropout=False, drop_rate=0.3, skip=False): # groups=1, dilation=1, norm_layer=None\n",
    "        super().__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        layers = [\n",
    "            Conv2d(in_channels, out_channels, kernel_size=kernel_size, padding=kernel_size-2, stride=stride, bias=False),  # , padding=kernel_size-1\n",
    "            BatchNorm2d(out_channels),\n",
    "            ReLU()\n",
    "        ]\n",
    "            \n",
    "        self.block = Sequential(*layers)\n",
    "        \n",
    "        self.skip = skip\n",
    "        if self.skip:\n",
    "            skip_layers = [\n",
    "                Conv2d(in_channels, out_channels, kernel_size=1, stride=1, bias=False),\n",
    "                BatchNorm2d(out_channels)\n",
    "            ]\n",
    "            self.skip_output = Sequential(*skip_layers)\n",
    "        \n",
    "        self.relu = ReLU()\n",
    "        \n",
    "        if dropout:\n",
    "            self.dropout=Dropout(drop_rate)\n",
    "        else:\n",
    "            self.dropout=None\n",
    "            \n",
    "    def forward(self,x):\n",
    "        identity = x\n",
    "        \n",
    "        out = self.block(x)\n",
    "        \n",
    "        if self.skip:\n",
    "            identity = self.skip_output(identity)\n",
    "            out += identity\n",
    "\n",
    "        out = self.relu(out)\n",
    "        \n",
    "        if self.dropout:\n",
    "            out = self.dropout(out)\n",
    "        \n",
    "        return out\n",
    "\n",
    "class Model(Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # Embedder\n",
    "        layers = [\n",
    "            BasicBlock(3, 64, dropout=True),\n",
    "            BasicBlock(64, 64),\n",
    "            MaxPool2d(2,2),\n",
    "            BasicBlock(64, 64),\n",
    "            MaxPool2d(2,2),\n",
    "            BasicBlock(64, 128, dropout=True),\n",
    "            BasicBlock(128, 128),\n",
    "            MaxPool2d(2,2),\n",
    "            BasicBlock(128, 256, dropout=True),\n",
    "            BasicBlock(256, 256),\n",
    "            MaxPool2d(2,2),\n",
    "            BasicBlock(256, 512),\n",
    "            MaxPool2d(2,2),\n",
    "            BasicBlock(512, 1024),\n",
    "            MaxPool2d(2,2)\n",
    "        ]\n",
    "        \n",
    "        # EMBEDDING\n",
    "        self.embed = Sequential(*layers)\n",
    "        \n",
    "        # CLASSIFICATION\n",
    "        self.classification = Linear(1024, 4000)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # EMBEDDING\n",
    "        out = self.embed(x)\n",
    "        embedding = torch.flatten(out,1)\n",
    "\n",
    "        classification = self.classification(embedding)\n",
    "                \n",
    "        return embedding, classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model_6_\n",
    "class BasicBlock(Module): \n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, dropout=False, drop_rate=0.3, skip=False): # groups=1, dilation=1, norm_layer=None\n",
    "        super().__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        layers = [\n",
    "            Conv2d(in_channels, out_channels, kernel_size=kernel_size, padding=kernel_size-2, stride=stride, bias=False),  # , padding=kernel_size-1\n",
    "            BatchNorm2d(out_channels),\n",
    "            LeakyReLU(),\n",
    "            Conv2d(out_channels, out_channels, kernel_size=kernel_size, padding=kernel_size-2, stride=stride, bias=False),  # , padding=kernel_size-1\n",
    "            BatchNorm2d(out_channels),\n",
    "            LeakyReLU()\n",
    "        ]\n",
    "            \n",
    "        self.block = Sequential(*layers)\n",
    "        \n",
    "        # Add padding to down sample\n",
    "        \n",
    "        self.skip = skip\n",
    "        if self.skip:\n",
    "            skip_layers = [\n",
    "                Conv2d(in_channels, out_channels, kernel_size=1, stride=1, bias=False),\n",
    "                BatchNorm2d(out_channels)\n",
    "            ]\n",
    "            self.skip_output = Sequential(*skip_layers)\n",
    "        \n",
    "        self.leakyrelu = LeakyReLU()\n",
    "        \n",
    "        if dropout:\n",
    "            self.dropout=Dropout(drop_rate)\n",
    "        else:\n",
    "            self.dropout=None\n",
    "            \n",
    "    def forward(self,x):\n",
    "        connection = x\n",
    "        \n",
    "        out = self.block(x)\n",
    "        \n",
    "        if self.skip:\n",
    "            connection = self.skip_output(connection)\n",
    "            out += connection\n",
    "\n",
    "        out = self.leakyrelu(out)\n",
    "        \n",
    "        if self.dropout:\n",
    "            out = self.dropout(out)\n",
    "        \n",
    "        return out\n",
    "\n",
    "# ResNet Influence, dropout from baseline\n",
    "class Model(Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # BIG BLOCK\n",
    "        layers = [\n",
    "            # HEAD\n",
    "            Conv2d(3, 64, kernel_size=7, padding=3, stride=1, bias=False),\n",
    "            MaxPool2d(2,2),\n",
    "            # BLOCK\n",
    "            BasicBlock(64, 64, dropout=True, skip=True),\n",
    "            BasicBlock(64, 64, skip=True),\n",
    "            BasicBlock(64, 64, skip=True),\n",
    "            MaxPool2d(2,2),\n",
    "            BasicBlock(64, 128, dropout=True, skip=True),\n",
    "            BasicBlock(128, 128, skip=True),\n",
    "            BasicBlock(128, 128, skip=True),\n",
    "            MaxPool2d(2,2),\n",
    "            BasicBlock(128, 256, dropout=True, skip=True),\n",
    "            BasicBlock(256, 256, skip=True),\n",
    "            BasicBlock(256, 256, skip=True),\n",
    "            BasicBlock(256, 256, skip=True),\n",
    "            MaxPool2d(2,2),\n",
    "            BasicBlock(256, 512, skip=True),\n",
    "            BasicBlock(512, 512, skip=True),\n",
    "            BasicBlock(512, 512, skip=True),\n",
    "            MaxPool2d(2,2),\n",
    "            BasicBlock(512, 1024, skip=True),\n",
    "            BasicBlock(1024, 1024, skip=True),\n",
    "            MaxPool2d(2,2)\n",
    "        ]\n",
    "\n",
    "        self.big_block = Sequential(*layers)\n",
    "\n",
    "        # EMBEDDING\n",
    "        self.embed = Linear(1024, 2048, bias=False)\n",
    "\n",
    "        \n",
    "        # CLASSIFICATION\n",
    "        class_layers = [\n",
    "            Linear(2048, 4096, bias=False),\n",
    "            BatchNorm1d(4096),\n",
    "            ReLU(),\n",
    "            Linear(4096, 4000, bias=False)\n",
    "        ]\n",
    "        self.classification = Sequential(*class_layers)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # BIG BLOCK\n",
    "        out = self.big_block(x)\n",
    "        \n",
    "        out = torch.flatten(out,1)\n",
    "\n",
    "        # EMBEDDING\n",
    "        embedding = self.embed(out)\n",
    "\n",
    "#         print(embedding.shape)\n",
    "        # CLASSIFY\n",
    "        classification = self.classification(embedding)\n",
    "                \n",
    "        return embedding, classification, out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model_15_ MobileNet - 1 skip over big block\n",
    "class BasicBlock(Module): \n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1): # groups=1, dilation=1, norm_layer=None\n",
    "        super().__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        layers = [\n",
    "            Conv2d(in_channels, out_channels, kernel_size=kernel_size, padding=kernel_size-2, stride=stride, bias=False),  # , padding=kernel_size-1\n",
    "            BatchNorm2d(out_channels),\n",
    "            ReLU(),\n",
    "        ]\n",
    "\n",
    "        self.block = Sequential(*layers)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        out = self.block(x)\n",
    "         \n",
    "        return out\n",
    "\n",
    "class DepthWiseBlock(Module): \n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1): # groups=1, dilation=1, norm_layer=None\n",
    "        super().__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        layers = [\n",
    "            Conv2d(in_channels, out_channels, kernel_size=kernel_size, padding=kernel_size-2, stride=stride, bias=False, groups=in_channels),\n",
    "            BatchNorm2d(out_channels),\n",
    "            ReLU(),\n",
    "            Conv2d(out_channels, out_channels, kernel_size=1, stride=1, bias=False),\n",
    "            BatchNorm2d(out_channels),\n",
    "            ReLU()\n",
    "        ]\n",
    "\n",
    "        self.block = Sequential(*layers)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        out = self.block(x)\n",
    "\n",
    "        return out\n",
    "# based on MobileNet\n",
    "class Model(Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Embedder\n",
    "        layers1 = [\n",
    "            BasicBlock(3, 32, stride=1), # originally stride=2, BasicBlock(3, 32, stride=1),\n",
    "            DepthWiseBlock(32, 32),\n",
    "            BasicBlock(32, 64),\n",
    "            DepthWiseBlock(64, 64, stride=2),\n",
    "            BasicBlock(64, 128),\n",
    "            DepthWiseBlock(128, 128),\n",
    "            BasicBlock(128, 128),\n",
    "            DepthWiseBlock(128, 128, stride=2),\n",
    "            BasicBlock(128, 256),\n",
    "            DepthWiseBlock(256, 256),\n",
    "            BasicBlock(256, 256),\n",
    "            DepthWiseBlock(256, 256, stride=2),\n",
    "            BasicBlock(256, 512)\n",
    "        ]\n",
    "        \n",
    "        BigBlock = []\n",
    "        for i in range(5):\n",
    "            BigBlock.append(DepthWiseBlock(512, 512))\n",
    "            BigBlock.append(BasicBlock(512, 512))\n",
    "            \n",
    "        layers2 = [\n",
    "            DepthWiseBlock(512, 512, stride=2),\n",
    "            BasicBlock(512, 1024),\n",
    "            DepthWiseBlock(1024, 1024, stride=2),\n",
    "            BasicBlock(1024, 1024)\n",
    "        ]\n",
    "        \n",
    "        self.layers1 = Sequential(*layers1)\n",
    "        self.BigBlock = Sequential(*BigBlock)\n",
    "        self.layers2 = Sequential(*layers2)\n",
    "        \n",
    "        # EMBEDDING\n",
    "        self.avgpool = AvgPool2d(2, stride=1)\n",
    "        \n",
    "        \n",
    "        self.classification = Linear(1024, 4000)    \n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Layers1\n",
    "        out = self.layers1(x)\n",
    "        \n",
    "        # Big Block\n",
    "        out = self.BigBlock(out) + out # skip connection\n",
    "        \n",
    "        # layers2\n",
    "        out = self.layers2(out)\n",
    "        \n",
    "        # EMBEDDING\n",
    "        out = self.avgpool(out)\n",
    "        embedding = torch.flatten(out,1)\n",
    "        \n",
    "        classification = self.classification(embedding)\n",
    "        \n",
    "        return embedding, classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model_17_ MobileNet2\n",
    "class BasicBlock(Module): \n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1): # groups=1, dilation=1, norm_layer=None\n",
    "        super().__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        layers = [\n",
    "            Conv2d(in_channels, out_channels, kernel_size=kernel_size, padding=kernel_size-2, stride=stride, bias=False),  # , padding=kernel_size-1\n",
    "            BatchNorm2d(out_channels),\n",
    "            ReLU(),\n",
    "        ]\n",
    "\n",
    "        self.block = Sequential(*layers)\n",
    "        \n",
    "#         # Add padding to down sample\n",
    "#         skip_layers = [\n",
    "#             Conv2d(in_channels, out_channels, kernel_size=1, stride=1, bias=False),\n",
    "#             BatchNorm2d(out_channels)\n",
    "#         ]\n",
    "#         self.skip = Sequential(*skip_layers)\n",
    "        \n",
    "#         self.relu = ReLU()\n",
    "\n",
    "    def forward(self,x):\n",
    "#         identity = x\n",
    "        \n",
    "        out = self.block(x)\n",
    "         \n",
    "#         identity = self.skip(identity)\n",
    "#         out += identity\n",
    "\n",
    "#         return self.relu(out)\n",
    "        return out\n",
    "\n",
    "class DepthWiseBlock(Module): \n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1): # groups=1, dilation=1, norm_layer=None\n",
    "        super().__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        layers = [\n",
    "            Conv2d(in_channels, out_channels, kernel_size=kernel_size, padding=kernel_size-2, stride=stride, bias=False, groups=in_channels),\n",
    "            BatchNorm2d(out_channels),\n",
    "            ReLU(),\n",
    "            Conv2d(out_channels, out_channels, kernel_size=1, stride=1, bias=False),\n",
    "            BatchNorm2d(out_channels),\n",
    "            ReLU()\n",
    "        ]\n",
    "\n",
    "        self.block = Sequential(*layers)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        out = self.block(x)\n",
    "\n",
    "        return out\n",
    "# based on MobileNet\n",
    "class Model(Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Embedder\n",
    "        layers1 = [\n",
    "            BasicBlock(3, 32, stride=1), # originally stride=2, BasicBlock(3, 32, stride=1),\n",
    "            DepthWiseBlock(32, 32),\n",
    "            BasicBlock(32, 64),\n",
    "            DepthWiseBlock(64, 64, stride=2),\n",
    "            BasicBlock(64, 128),\n",
    "            DepthWiseBlock(128, 128),\n",
    "            BasicBlock(128, 128),\n",
    "            DepthWiseBlock(128, 128, stride=2),\n",
    "            BasicBlock(128, 256),\n",
    "            DepthWiseBlock(256, 256),\n",
    "            BasicBlock(256, 256),\n",
    "            DepthWiseBlock(256, 256, stride=2),\n",
    "            BasicBlock(256, 512)\n",
    "        ]\n",
    "        \n",
    "        BigBlock = []\n",
    "        for i in range(5):\n",
    "            BigBlock.append(DepthWiseBlock(512, 512))\n",
    "            BigBlock.append(BasicBlock(512, 512))\n",
    "            \n",
    "        layers2 = [\n",
    "            DepthWiseBlock(512, 1024, stride=2),\n",
    "            BasicBlock(1024, 1024),\n",
    "            DepthWiseBlock(1024, 2048, stride=2),\n",
    "            BasicBlock(2048, 2048)\n",
    "        ]\n",
    "        \n",
    "        self.layers1 = Sequential(*layers1)\n",
    "        self.BigBlock = Sequential(*BigBlock)\n",
    "        self.layers2 = Sequential(*layers2)\n",
    "        \n",
    "        # EMBEDDING\n",
    "        self.avgpool = AvgPool2d(2, stride=1)\n",
    "        \n",
    "#         classification_layers = [\n",
    "#             Linear(1024, 2048),            # DO THIS AFTER 2048 embedding then can try contrastive loss after training this\n",
    "#             BatchNorm1d(),\n",
    "#             ReLU(),\n",
    "#             Linear(2048, 4000)\n",
    "#         ]\n",
    "        self.classification = Linear(2048, 4000) # Sequential(*classification_layers) # \n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Layers1\n",
    "        out = self.layers1(x)\n",
    "        \n",
    "        # Big Block\n",
    "        out = self.BigBlock(out) + out # skip connection\n",
    "        \n",
    "        # layers2\n",
    "        out = self.layers2(out)\n",
    "        \n",
    "        # EMBEDDING\n",
    "        out = self.avgpool(out)\n",
    "        embedding = torch.flatten(out,1)\n",
    "        \n",
    "        classification = self.classification(embedding)\n",
    "        \n",
    "        return embedding, classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Needs Work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model_1_ MobileNet\n",
    "class BasicBlock(Module): \n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1): # groups=1, dilation=1, norm_layer=None\n",
    "        super().__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        layers = [\n",
    "            Conv2d(in_channels, out_channels, kernel_size=kernel_size, padding=kernel_size-2, stride=stride, bias=False),  # , padding=kernel_size-1\n",
    "            BatchNorm2d(out_channels),\n",
    "            ReLU(),\n",
    "        ]\n",
    "\n",
    "        self.block = Sequential(*layers)\n",
    "        \n",
    "#         # Add padding to down sample\n",
    "#         skip_layers = [\n",
    "#             Conv2d(in_channels, out_channels, kernel_size=1, stride=1, bias=False),\n",
    "#             BatchNorm2d(out_channels)\n",
    "#         ]\n",
    "#         self.skip = Sequential(*skip_layers)\n",
    "        \n",
    "#         self.relu = ReLU()\n",
    "\n",
    "    def forward(self,x):\n",
    "#         identity = x\n",
    "        \n",
    "        out = self.block(x)\n",
    "         \n",
    "#         identity = self.skip(identity)\n",
    "#         out += identity\n",
    "\n",
    "#         return self.relu(out)\n",
    "        return out\n",
    "\n",
    "class DepthWiseBlock(Module): \n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1): # groups=1, dilation=1, norm_layer=None\n",
    "        super().__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        layers = [\n",
    "            Conv2d(in_channels, out_channels, kernel_size=kernel_size, padding=kernel_size-2, stride=stride, bias=False, groups=in_channels),\n",
    "            BatchNorm2d(out_channels),\n",
    "            ReLU(),\n",
    "            Conv2d(out_channels, out_channels, kernel_size=1, stride=1, bias=False),\n",
    "            BatchNorm2d(out_channels),\n",
    "            ReLU()\n",
    "        ]\n",
    "\n",
    "        self.block = Sequential(*layers)\n",
    "        \n",
    "        # Add padding to down sample\n",
    "#         skip_layers = [\n",
    "#             Conv2d(in_channels, out_channels, kernel_size=1, stride=1, bias=False),\n",
    "#             BatchNorm2d(out_channels)\n",
    "#         ]\n",
    "#         self.skip = Sequential(*skip_layers)\n",
    "        \n",
    "#         self.relu = ReLU()\n",
    "\n",
    "    def forward(self,x):\n",
    "#         identity = x\n",
    "        \n",
    "        out = self.block(x)\n",
    "         \n",
    "#         identity = self.skip(identity)\n",
    "#         out += identity\n",
    "\n",
    "#         return self.relu(out)\n",
    "        return out\n",
    "\n",
    "# based on MobileNet\n",
    "class Model(Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # HEAD: maybe have a head with max pooling\n",
    "        head = [\n",
    "            Conv2d(3, 32, kernel_size=3, padding=1, bias=False),\n",
    "            BatchNorm2d(32),\n",
    "            ReLU(),\n",
    "            Dropout()\n",
    "        ]\n",
    "        self.head = Sequential(*head)\n",
    "        \n",
    "        BigBlock = []\n",
    "        for i in range(5):\n",
    "            BigBlock.append(DepthWiseBlock(512, 512))\n",
    "            BigBlock.append(BasicBlock(512, 512))\n",
    "        \n",
    "        # Embedder\n",
    "        layers = [\n",
    "            BasicBlock(32, 32, stride=1), # originally stride=2, BasicBlock(3, 32, stride=1),\n",
    "            DepthWiseBlock(32, 32),\n",
    "            BasicBlock(32, 64),\n",
    "            DepthWiseBlock(64, 64, stride=2),\n",
    "            BasicBlock(64, 128),\n",
    "            DepthWiseBlock(128, 128),\n",
    "            BasicBlock(128, 128),\n",
    "            DepthWiseBlock(128, 128, stride=2),\n",
    "            BasicBlock(128, 256),\n",
    "            DepthWiseBlock(256, 256),\n",
    "            BasicBlock(256, 256),\n",
    "            DepthWiseBlock(256, 256, stride=2),\n",
    "            BasicBlock(256, 512),\n",
    "            *BigBlock,\n",
    "            DepthWiseBlock(512, 512, stride=2),\n",
    "            BasicBlock(512, 1024),\n",
    "            DepthWiseBlock(1024, 1024, stride=2),\n",
    "            BasicBlock(1024, 1024)\n",
    "#             AvgPool2d()\n",
    "        ]\n",
    "        \n",
    "        self.layers = Sequential(*layers)\n",
    "        \n",
    "        # EMBEDDING\n",
    "        self.avgpool = AvgPool2d(2, stride=1)  # TODO: Check if this works\n",
    "        self.embed = Linear(1024, 1024)\n",
    "        \n",
    "        # CLASSIFICATION\n",
    "        classification = [\n",
    "            Linear(1024, 1024),          # TODO: Is this okay?\n",
    "            BatchNorm1d(1024),\n",
    "            ReLU(),\n",
    "            Linear(1024, 4000)\n",
    "        ]\n",
    "        self.classification = Sequential(*classification)\n",
    "    \n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Layers\n",
    "        out = self.layers(x)\n",
    "        \n",
    "        # EMBEDDING\n",
    "        out = self.avgpool(out)\n",
    "        out = torch.flatten(out,1)\n",
    "        embedding = self.embed(out)\n",
    "        \n",
    "        classification = self.classification(embedding)\n",
    "        \n",
    "        return embedding, classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# based on code from: https://github.com/pytorch/vision/blob/master/torchvision/models/resnet.py\n",
    "class BasicBlock(Module): \n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1): # groups=1, dilation=1, norm_layer=None\n",
    "        super().__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        layers = [\n",
    "            Conv2d(in_channels, out_channels, kernel_size=kernel_size, padding=kernel_size-2, stride=stride, bias=False),  # , padding=kernel_size-1\n",
    "            BatchNorm2d(out_channels),\n",
    "            ReLU(),\n",
    "            Conv2d(out_channels, out_channels, kernel_size=kernel_size, padding=kernel_size-2, stride=stride, bias=False), # , padding=kernel_size-1\n",
    "            BatchNorm2d(out_channels)\n",
    "        ]\n",
    "\n",
    "        self.block_preskip = Sequential(*layers)\n",
    "        \n",
    "        # Used for downsampling or updating x to be in same form/size as convolution\n",
    "        # Add padding to down sample\n",
    "        skip_layers = [\n",
    "            Conv2d(in_channels, out_channels, kernel_size=1, stride=1, bias=False),\n",
    "            BatchNorm2d(out_channels)\n",
    "        ]\n",
    "        self.skip = Sequential(*skip_layers)\n",
    "        \n",
    "        self.relu = ReLU()\n",
    "\n",
    "    def forward(self,x):\n",
    "        identity = x\n",
    "        \n",
    "        out = self.block_preskip(x)\n",
    "         \n",
    "        # TODO: implement skip connections - TEST INDIVIDUALLY WITH FORWARD ON RANDOM INPUT\n",
    "#         if downsample is not None:\n",
    "#         if self.in_channels != self.out_channels:\n",
    "        identity = self.skip(identity)\n",
    "#     torch.Size([32, 128, 63, 63]) torch.Size([32, 128, 59, 59]) torch.Size([32, 64, 59, 59])\n",
    "        print(out.shape, identity.shape, x.shape)\n",
    "        out += identity\n",
    "\n",
    "        return self.relu(out)\n",
    "    \n",
    "# based on ResNet from: https://github.com/pytorch/vision/blob/master/torchvision/models/resnet.py\n",
    "class Model(Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # HEAD\n",
    "        head = [\n",
    "            Conv2d(3, 64, kernel_size=8, stride=1, padding=1, bias=False),\n",
    "            BatchNorm2d(64),\n",
    "            ReLU(),\n",
    "            MaxPool2d(kernel_size=3, stride=1, padding=1)\n",
    "        ]\n",
    "        self.head = Sequential(*head)\n",
    "        \n",
    "        # BASIC BLOCKS\n",
    "#         block = []\n",
    "        self.block1 = self._make_block(2, 64, 128)\n",
    "        self.block2 = self._make_block(2, 128, 256)\n",
    "        self.block3 = self._make_block(2, 256, 512)\n",
    "        self.block4 = self._make_block(2, 512, 512)\n",
    "        \n",
    "        # TAIL\n",
    "        self.avgpool = AdaptiveAvgPool2d((1,1))  # TODO: Check if this works\n",
    "        self.embed = Linear(512, 1024)\n",
    "        \n",
    "        # CLASSIFICATION\n",
    "        classification = [\n",
    "            Linear(1024, 1024),          # TODO: Is this okay?\n",
    "            BatchNorm1d(1024),\n",
    "            ReLU(),\n",
    "            Linear(1024, 4000)\n",
    "        ]\n",
    "        self.classification = Sequential(*classification)\n",
    "    \n",
    "    def _make_block(self, blocks, in_channel, out_channel, kernel_size=3, stride=1):\n",
    "        layers = []\n",
    "        layers.append(BasicBlock(in_channel, out_channel, kernel_size=kernel_size, stride=stride))\n",
    "        for i in range(1,blocks):\n",
    "            layers.append(BasicBlock(out_channel, out_channel, kernel_size=kernel_size, stride=stride))\n",
    "         \n",
    "        return Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.head(x)\n",
    "        \n",
    "        out = self.block1(out)\n",
    "        out = self.block2(out)\n",
    "        out = self.block3(out)\n",
    "        out = self.block4(out)\n",
    "        \n",
    "        out = self.avgpool(out)\n",
    "        out = torch.flatten(out, 1)\n",
    "        embedding = self.embed(out)\n",
    "        \n",
    "        classification = self.classification(embedding)\n",
    "        \n",
    "        return embedding, classification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model_7_, Model_10_\n",
    "class BasicBlock(Module): \n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, dropout=False, drop_rate=0.3): # groups=1, dilation=1, norm_layer=None\n",
    "        super().__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        layers = [\n",
    "            Conv2d(in_channels, out_channels, kernel_size=kernel_size, padding=kernel_size-2, stride=stride, bias=False),  # , padding=kernel_size-1\n",
    "            BatchNorm2d(out_channels),\n",
    "            ReLU()\n",
    "        ]\n",
    "        \n",
    "        if dropout:\n",
    "            layers.append(Dropout(drop_rate))\n",
    "            \n",
    "        self.block = Sequential(*layers)\n",
    "\n",
    "    def forward(self,x):\n",
    "        out = self.block(x)\n",
    "         \n",
    "        return out\n",
    "\n",
    "# Model_7_, Model_10_\n",
    "class Model(Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # Embedder\n",
    "        layers = [\n",
    "            BasicBlock(3, 64, dropout=True),\n",
    "            BasicBlock(64, 64),\n",
    "            MaxPool2d(2,2),\n",
    "            BasicBlock(64, 64, dropout=True),\n",
    "            BasicBlock(64, 64),\n",
    "            MaxPool2d(2,2),\n",
    "            BasicBlock(64, 128, dropout=True),\n",
    "            BasicBlock(128, 128),\n",
    "            MaxPool2d(2,2),\n",
    "            BasicBlock(128, 256),\n",
    "            BasicBlock(256, 256),\n",
    "            BasicBlock(256, 256),\n",
    "            BasicBlock(256, 256),\n",
    "            MaxPool2d(2,2),\n",
    "            BasicBlock(256, 512),\n",
    "            MaxPool2d(2,2),\n",
    "            BasicBlock(512, 1024),\n",
    "            MaxPool2d(2,2)\n",
    "        ]\n",
    "        \n",
    "        # EMBEDDING\n",
    "        self.embed = Sequential(*layers)\n",
    "#         self.embed = Linear(256, 1000)\n",
    "\n",
    "        \n",
    "        # CLASSIFICATION\n",
    "        self.classification = Linear(1024, 4000)\n",
    "        #             Linear(1024, 4000)\n",
    "\n",
    "        \n",
    "#         ]\n",
    "#         self.classification = Sequential(*classification)\n",
    "    \n",
    "    \n",
    "    def forward(self, x):\n",
    "        # EMBEDDING\n",
    "        out = self.embed(x)\n",
    "        embedding = torch.flatten(out,1)\n",
    "    \n",
    "        classification = self.classification(embedding)\n",
    "                \n",
    "        return embedding, classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# can't really make siamese models like this because the backwards will not be identical\n",
    "val_accuracies=[]\n",
    "model_number=0\n",
    "prev_auc = 0\n",
    "running_max = ['',0.]\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    ti = time()\n",
    "    model1.train()\n",
    "    model2.train()\n",
    "    for i, (x,y) in enumerate(ver_train_loader):\n",
    "        optimizer1.zero_grad()\n",
    "        optimizer2.zero_grad()\n",
    "\n",
    "        img1 = x[0].to(device)\n",
    "        img2 = x[1].to(device)\n",
    "#         y = y.reshape(-1).to(device) # need to turn to row\n",
    "\n",
    "        embedding1 = model1(img1)[0]\n",
    "        embedding2 = model1(img2)[0]\n",
    "        \n",
    "        loss = criterion(embedding1, embedding2, y)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer1.step()\n",
    "        optimizer2.step()\n",
    "\n",
    "        # progress\n",
    "        if i%10==0:\n",
    "            print('Epoch:', epoch, '| Iteration:', i, end='\\r')\n",
    "\n",
    "    \n",
    "    # Deallocate memory in GPU\n",
    "    torch.cuda.empty_cache()\n",
    "    del x\n",
    "    del y\n",
    "\n",
    "    # validation\n",
    "    accuracy = validate(model1, val_loader, val_dataset)\n",
    "    AUC = verify(model1, ver_val_loader, sub_name)\n",
    "    val_accuracies.append(accuracy)\n",
    "    \n",
    "    if (accuracy > 0.65 or AUC > 0.90):\n",
    "        save_state(AUC, accuracy, model_number, model1, ver_val_loader_args, device, NUM_EPOCHS, learning_rate, optimizer1, criterion)\n",
    "    model_number+=1\n",
    "\n",
    "    scheduler1.step()\n",
    "    scheduler2.step()\n",
    "\n",
    "    \n",
    "    print(\"Epoch\", epoch, \"Accuracy:\", accuracy, \"AUC:\", AUC)\n",
    "    if prev_auc == 0:\n",
    "        print(\"\\tImprovement:\", AUC-prev_auc)\n",
    "    else:\n",
    "        print(\"\\tImprovement:\", AUC-prev_auc, \"| Percent Improvement (times):\", 100*AUC/prev_auc, '%')\n",
    "    # tracking running best AUC\n",
    "    if running_max[1]<AUC:\n",
    "        running_max[0]='Model_' + str(RUN_NUMBER) + '_' + str(epoch)\n",
    "        running_max[1]=AUC\n",
    "    tf=time()\n",
    "    print(\"Time for epoch:\", tf-ti)\n",
    "    print('   Running Max:', *running_max,'\\n')\n",
    "\n",
    "    prev_auc = AUC"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
